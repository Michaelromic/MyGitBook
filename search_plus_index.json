{"./":{"url":"./","title":"About","keywords":"","body":"MyGitBook Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"linux/1-1.html":{"url":"linux/1-1.html","title":"1-1","keywords":"","body":" Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"nodejs/koa.html":{"url":"nodejs/koa.html","title":"koa","keywords":"","body":"转自： Koa 框架教程 Node 主要用在开发 Web 应用。这决定了使用 Node，往往离不开 Web 应用框架。 Koa 就是一种简单好用的 Web 框架。它的特点是优雅、简洁、表达力强、自由度高。本身代码只有1000多行，所有功能都通过插件实现，很符合 Unix 哲学。 一、基本用法 1.1 架设 HTTP 服务 只要三行代码，就可以用 Koa 架设一个 HTTP 服务。 // demos/01.js const Koa = require('koa'); const app = new Koa(); app.listen(3000); 运行这个脚本。 $ node demos/01.js 打开浏览器，访问 http://127.0.0.1:3000 。你会看到页面显示\"Not Found\"，表示没有发现任何内容。这是因为我们并没有告诉 Koa 应该显示什么内容。 1.2 Context 对象 Koa 提供一个 Context 对象，表示一次对话的上下文（包括 HTTP 请求和 HTTP 回复）。通过加工这个对象，就可以控制返回给用户的内容。 Context.response.body属性就是发送给用户的内容。请看下面的例子（完整的代码看这里）。 // demos/02.js const Koa = require('koa'); const app = new Koa(); const main = ctx => { ctx.response.body = 'Hello World'; }; app.use(main); app.listen(3000); 上面代码中，main函数用来设置ctx.response.body。然后，使用app.use方法加载main函数。 你可能已经猜到了，ctx.response代表 HTTP Response。同样地，ctx.request代表 HTTP Request。 运行这个 demo。 $ node demos/02.js 访问 http://127.0.0.1:3000 ，现在就可以看到\"Hello World\"了。 1.3 HTTP Response 的类型 Koa 默认的返回类型是text/plain，如果想返回其他类型的内容，可以先用ctx.request.accepts判断一下，客户端希望接受什么数据（根据 HTTP Request 的Accept字段），然后使用ctx.response.type指定返回类型。请看下面的例子（完整代码看这里）。 // demos/03.js const main = ctx => { if (ctx.request.accepts('xml')) { ctx.response.type = 'xml'; ctx.response.body = 'Hello World'; } else if (ctx.request.accepts('json')) { ctx.response.type = 'json'; ctx.response.body = { data: 'Hello World' }; } else if (ctx.request.accepts('html')) { ctx.response.type = 'html'; ctx.response.body = 'Hello World'; } else { ctx.response.type = 'text'; ctx.response.body = 'Hello World'; } }; 运行这个 demo。 $ node demos/03.js 访问 http://127.0.0.1:3000 ，现在看到的就是一个 XML 文档了。 1.4 网页模板 实际开发中，返回给用户的网页往往都写成模板文件。我们可以让 Koa 先读取模板文件，然后将这个模板返回给用户。请看下面的例子（完整代码看这里）。 // demos/04.js const fs = require('fs'); const main = ctx => { ctx.response.type = 'html'; ctx.response.body = fs.createReadStream('./demos/template.html'); }; 运行这个 Demo。 $ node demos/04.js 访问 http://127.0.0.1:3000 ，看到的就是模板文件的内容了。 二、路由 2.1 原生路由 网站一般都有多个页面。通过ctx.request.path可以获取用户请求的路径，由此实现简单的路由。请看下面的例子（完整代码看这里）。 // demos/05.js const main = ctx => { if (ctx.request.path !== '/') { ctx.response.type = 'html'; ctx.response.body = 'Index Page'; } else { ctx.response.body = 'Hello World'; } }; 运行这个 demo。 $ node demos/05.js 访问 http://127.0.0.1:3000/about ，可以看到一个链接，点击后就跳到首页。 2.2 koa-route 模块 原生路由用起来不太方便，我们可以使用封装好的koa-route模块。请看下面的例子（完整代码看这里）。 // demos/06.js const route = require('koa-route'); const about = ctx => { ctx.response.type = 'html'; ctx.response.body = 'Index Page'; }; const main = ctx => { ctx.response.body = 'Hello World'; }; app.use(route.get('/', main)); app.use(route.get('/about', about)); 上面代码中，根路径/的处理函数是main，/about路径的处理函数是about。 运行这个 demo。 $ node demos/06.js 访问 http://127.0.0.1:3000/about ，效果与上一个例子完全相同。 2.3 静态资源 如果网站提供静态资源（图片、字体、样式表、脚本......），为它们一个个写路由就很麻烦，也没必要。koa-static模块封装了这部分的请求。请看下面的例子（完整代码看这里）。 // demos/12.js const path = require('path'); const serve = require('koa-static'); const main = serve(path.join(__dirname)); app.use(main); 运行这个 Demo。 $ node demos/12.js 访问 http://127.0.0.1:3000/12.js，在浏览器里就可以看到这个脚本的内容。 2.4 重定向 有些场合，服务器需要重定向（redirect）访问请求。比如，用户登陆以后，将他重定向到登陆前的页面。ctx.response.redirect()方法可以发出一个302跳转，将用户导向另一个路由。请看下面的例子（完整代码看这里）。 // demos/13.js const redirect = ctx => { ctx.response.redirect('/'); ctx.response.body = 'Index Page'; }; app.use(route.get('/redirect', redirect)); 运行这个 demo。 $ node demos/13.js 访问 http://127.0.0.1:3000/redirect ，浏览器会将用户导向根路由。 三、中间件 3.1 Logger 功能 Koa 的最大特色，也是最重要的一个设计，就是中间件（middleware）。为了理解中间件，我们先看一下 Logger （打印日志）功能的实现。 最简单的写法就是在main函数里面增加一行（完整代码看这里）。 // demos/07.js const main = ctx => { console.log(${Date.now()} ${ctx.request.method} ${ctx.request.url}); ctx.response.body = 'Hello World'; }; 运行这个 Demo。 $ node demos/07.js 访问 http://127.0.0.1:3000 ，命令行就会输出日志。 1502144902843 GET / 3.2 中间件的概念 上一个例子里面的 Logger 功能，可以拆分成一个独立函数（完整代码看这里）。 // demos/08.js const logger = (ctx, next) => { console.log(${Date.now()} ${ctx.request.method} ${ctx.request.url}); next(); } app.use(logger); 像上面代码中的logger函数就叫做\"中间件\"（middleware），因为它处在 HTTP Request 和 HTTP Response 中间，用来实现某种中间功能。app.use()用来加载中间件。 基本上，Koa 所有的功能都是通过中间件实现的，前面例子里面的main也是中间件。每个中间件默认接受两个参数，第一个参数是 Context 对象，第二个参数是next函数。只要调用next函数，就可以把执行权转交给下一个中间件。 运行这个 demo。 $ node demos/08.js 访问 http://127.0.0.1:3000 ，命令行窗口会显示与上一个例子相同的日志输出。 3.3 中间件栈 多个中间件会形成一个栈结构（middle stack），以\"先进后出\"（first-in-last-out）的顺序执行。 最外层的中间件首先执行。 调用next函数，把执行权交给下一个中间件。 ... 最内层的中间件最后执行。 执行结束后，把执行权交回上一层的中间件。 ... 最外层的中间件收回执行权之后，执行next函数后面的代码。 请看下面的例子（完整代码看这里）。 // demos/09.js const one = (ctx, next) => { console.log('>> one'); next(); console.log(' const two = (ctx, next) => { console.log('>> two'); next(); console.log(' const three = (ctx, next) => { console.log('>> three'); next(); console.log(' app.use(one); app.use(two); app.use(three); 运行这个 demo。 $ node demos/09.js 访问 http://127.0.0.1:3000 ，命令行窗口会有如下输出。 >> one two three 如果中间件内部没有调用next函数，那么执行权就不会传递下去。作为练习，你可以将two函数里面next()这一行注释掉再执行，看看会有什么结果。 3.4 异步中间件 迄今为止，所有例子的中间件都是同步的，不包含异步操作。如果有异步操作（比如读取数据库），中间件就必须写成 async 函数。请看下面的例子（完整代码看这里）。 // demos/10.js const fs = require('fs.promised'); const Koa = require('koa'); const app = new Koa(); const main = async function (ctx, next) { ctx.response.type = 'html'; ctx.response.body = await fs.readFile('./demos/template.html', 'utf8'); }; app.use(main); app.listen(3000); 上面代码中，fs.readFile是一个异步操作，必须写成await fs.readFile()，然后中间件必须写成 async 函数。 运行这个 demo。 $ node demos/10.js 访问 http://127.0.0.1:3000 ，就可以看到模板文件的内容。 3.5 中间件的合成 koa-compose模块可以将多个中间件合成为一个。请看下面的例子（完整代码看这里）。 // demos/11.js const compose = require('koa-compose'); const logger = (ctx, next) => { console.log(${Date.now()} ${ctx.request.method} ${ctx.request.url}); next(); } const main = ctx => { ctx.response.body = 'Hello World'; }; const middlewares = compose([logger, main]); app.use(middlewares); 运行这个 demo。 $ node demos/11.js 访问 http://127.0.0.1:3000 ，就可以在命令行窗口看到日志信息。 四、错误处理 4.1 500 错误 如果代码运行过程中发生错误，我们需要把错误信息返回给用户。HTTP 协定约定这时要返回500状态码。Koa 提供了ctx.throw()方法，用来抛出错误，ctx.throw(500)就是抛出500错误。请看下面的例子（完整代码看这里）。 // demos/14.js const main = ctx => { ctx.throw(500); }; 运行这个 demo。 $ node demos/14.js 访问 http://127.0.0.1:3000，你会看到一个500错误页\"Internal Server Error\"。 4.2 404错误 如果将ctx.response.status设置成404，就相当于ctx.throw(404)，返回404错误。请看下面的例子（完整代码看这里）。 // demos/15.js const main = ctx => { ctx.response.status = 404; ctx.response.body = 'Page Not Found'; }; 运行这个 demo。 $ node demos/15.js 访问 http://127.0.0.1:3000 ，你就看到一个404页面\"Page Not Found\"。 4.3 处理错误的中间件 为了方便处理错误，最好使用try...catch将其捕获。但是，为每个中间件都写try...catch太麻烦，我们可以让最外层的中间件，负责所有中间件的错误处理。请看下面的例子（完整代码看这里）。 // demos/16.js const handler = async (ctx, next) => { try { await next(); } catch (err) { ctx.response.status = err.statusCode || err.status || 500; ctx.response.body = { message: err.message }; } }; const main = ctx => { ctx.throw(500); }; app.use(handler); app.use(main); 运行这个 demo。 $ node demos/16.js 访问 http://127.0.0.1:3000 ，你会看到一个500页，里面有报错提示 {\"message\":\"Internal Server Error\"}。 4.4 error 事件的监听 运行过程中一旦出错，Koa 会触发一个error事件。监听这个事件，也可以处理错误。请看下面的例子（完整代码看这里）。 // demos/17.js const main = ctx => { ctx.throw(500); }; app.on('error', (err, ctx) => console.error('server error', err); ); 运行这个 demo。 $ node demos/17.js 访问 http://127.0.0.1:3000 ，你会在命令行窗口看到\"server error xxx\"。 4.5 释放 error 事件 需要注意的是，如果错误被try...catch捕获，就不会触发error事件。这时，必须调用ctx.app.emit()，手动释放error事件，才能让监听函数生效。请看下面的例子（完整代码看这里）。 // demos/18.js` const handler = async (ctx, next) => { try { await next(); } catch (err) { ctx.response.status = err.statusCode || err.status || 500; ctx.response.type = 'html'; ctx.response.body = 'Something wrong, please contact administrator.'; ctx.app.emit('error', err, ctx); } }; const main = ctx => { ctx.throw(500); }; app.on('error', function(err) { console.log('logging error ', err.message); console.log(err); }); 上面代码中，main函数抛出错误，被handler函数捕获。catch代码块里面使用ctx.app.emit()手动释放error事件，才能让监听函数监听到。 运行这个 demo。 $ node demos/18.js 访问 http://127.0.0.1:3000 ，你会在命令行窗口看到logging error。 五、Web App 的功能 5.1 Cookies ctx.cookies用来读写 Cookie。请看下面的例子（完整代码看这里）。 // demos/19.js const main = function(ctx) { const n = Number(ctx.cookies.get('view') || 0) + 1; ctx.cookies.set('view', n); ctx.response.body = n + ' views'; } 运行这个 demo。 $ node demos/19.js 访问 http://127.0.0.1:3000 ，你会看到1 views。刷新一次页面，就变成了2 views。再刷新，每次都会计数增加1。 5.2 表单 Web 应用离不开处理表单。本质上，表单就是 POST 方法发送到服务器的键值对。koa-body模块可以用来从 POST 请求的数据体里面提取键值对。请看下面的例子（完整代码看这里）。 // demos/20.js const koaBody = require('koa-body'); const main = async function(ctx) { const body = ctx.request.body; if (!body.name) ctx.throw(400, '.name required'); ctx.body = { name: body.name }; }; app.use(koaBody()); 运行这个 demo。 $ node demos/20.js 打开另一个命令行窗口，运行下面的命令。 $ curl -X POST --data \"name=Jack\" 127.0.0.1:3000 {\"name\":\"Jack\"} $ curl -X POST --data \"name\" 127.0.0.1:3000 name required 上面代码使用 POST 方法向服务器发送一个键值对，会被正确解析。如果发送的数据不正确，就会收到错误提示。 2.3 文件上传 koa-body模块还可以用来处理文件上传。请看下面的例子（完整代码看这里）。 // demos/21.js const os = require('os'); const path = require('path'); const koaBody = require('koa-body'); const main = async function(ctx) { const tmpdir = os.tmpdir(); const filePaths = []; const files = ctx.request.body.files || {}; for (let key in files) { const file = files[key]; const filePath = path.join(tmpdir, file.name); const reader = fs.createReadStream(file.path); const writer = fs.createWriteStream(filePath); reader.pipe(writer); filePaths.push(filePath); } ctx.body = filePaths; }; app.use(koaBody({ multipart: true })); 运行这个 demo。 $ node demos/21.js 打开另一个命令行窗口，运行下面的命令，上传一个文件。注意，/path/to/file要更换为真实的文件路径。 $ curl --form upload=@/path/to/file http://127.0.0.1:3000 [\"/tmp/file\"] 六、参考链接 koa workshop kick-off-koa Koa Examples Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"nodejs/const,var,let区别.html":{"url":"nodejs/const,var,let区别.html","title":"const,var,let区别","keywords":"","body":"js中const,var,let区别 js中三种定义变量的方式const， var， let的区别。 1. const定义的变量不可以修改，而且必须初始化。 const b = 2;//正确 // const b;//错误，必须初始化 console.log('函数外const定义b：' + b);//有输出值 // b = 5; // console.log('函数外修改const定义b：' + b);//无法输出 2. var定义的变量可以修改，如果不初始化会输出undefined，不会报错。 var a = 1; // var a;//不会报错 console.log('函数外var定义a：' + a);//可以输出a=1 function change(){ a = 4; console.log('函数内var定义a：' + a);//可以输出a=4 } change(); console.log('函数调用后var定义a为函数内部修改值：' + a);//可以输出a=4 3. let是块级作用域，函数内部使用let定义后，对函数外部无影响。 let c = 3; console.log('函数外let定义c：' + c);//输出c=3 function change(){ let c = 6; console.log('函数内let定义c：' + c);//输出c=6 } change(); console.log('函数调用后let定义c不受函数内部定义影响：' + c);//输出c=3 Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"nodejs/node-rsa加密解密.html":{"url":"nodejs/node-rsa加密解密.html","title":"node-rsa加密解密","keywords":"","body":"node-rsa加密解密用法 生成公钥私钥 var NodeRSA = require('node-rsa') var fs = require('fs') function generator() { var key = new NodeRSA({ b: 512 }) key.setOptions({ encryptionScheme: 'pkcs1' }) var privatePem = key.exportKey('pkcs1-private-pem') var publicPem = key.exportKey('pkcs1-public-pem') fs.writeFile('./pem/public.pem', publicPem, (err) => { if (err) throw err console.log('公钥已保存！') }) fs.writeFile('./pem/private.pem', privatePem, (err) => { if (err) throw err console.log('私钥已保存！') }) } 加密 加密 \"hello world\" 这个字符串 function encrypt() { fs.readFile('./pem/private.pem', function (err, data) { var key = new NodeRSA(data); let cipherText = key.encryptPrivate('hello world', 'base64'); console.log(cipherText); }); } 解密 把上一步加密获得的密文复制粘贴到下面要解密的方法内 function decrypt() { fs.readFile('./pem/public.pem', function (err, data) { var key = new NodeRSA(data); let rawText = key.decryptPublic('fH1aVCUceJYVvt1tZ7WYc1Dh5dVCd952GY5CX283V/wK2229FLgT9WfRNAPMjbTtwL9ghVeYD4Lsi6yM1t4OqA==', 'utf8'); console.log(rawText); }); } 上面的例子是私钥加密，公钥解密，下面自测，用公钥加密，私钥解密 const fs = require(\"fs\"); const NodeRSA = require('node-rsa') function rsaEncrypt(data) { return new Promise((resolve,reject)=>{ fs.readFile('./rsa_key/rsa_public_key.pem', function (err, pubkey) { if(err){ reject(console.log(err)) } else { const key = new NodeRSA(pubkey); const cipherText = key.encrypt(data, 'base64'); console.log(data); console.log(cipherText); console.log(\"111\"); resolve(cipherText); } }); }) } function rsaDecrypt(data) { fs.readFile('./rsa_key/rsa_private_key.pem', function (err, privkey) { var key = new NodeRSA(privkey); let rawText = key.decrypt(data, 'utf8'); console.log(rawText); }); } async function testrsa() { const data = \"cUw4fDSUDXYbFSDp5WUBYhpaKb3cdzaQyBFJVzAH3YJFgNb5dTSM\"; const result = await rsaEncrypt(data); console.log(\"222\"); console.log(result); rsaDecrypt(result); } testrsa(); Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"nodejs/node-mysql.html":{"url":"nodejs/node-mysql.html","title":"node-mysql","keywords":"","body":"Node连接MySQL并封装其增删查改 调用方法 1. 连接Mysql const mysql = require('mysql'); let connection = mysql.createConnection({ host : 'localhost', user : 'root', password : 'password', database : 'test' }); connection.connect(function(err) { if (err) { console.error('连接失败: ' + err.stack); return; } console.log('连接成功 id ' + connection.threadId); }); 2. 查询 connection.query('SELECT * FROM t_user WHERE username = \"whg\"', (err, results, fields) => { if(err){ console.log(err); } console.log(results); }) 3. 添加 connection.query('INSERT INTO t_user(username, pass) VALUES(?, ?)',['whg', '123'], (err, results) => { if(err){ console.log(err); } console.log(results); }) 4. 删除 connection.query('DELETE FROM t_user WHERE id = 1', (err, results) => { if(err){ console.log(err); } console.log(results); }) 5. 更新 connection.query('UPDATE t_user SET pass = \"321\" WHERE username = \"whg\"', (err, results) => { if(err){ console.log(err); } console.log(results); }) 6. 结束连接 connection.end(function(err) { }); connection.destroy(); 封装 1. 数据库配置文件 //配置链接数据库参数 module.exports = { host : 'localhost', port : 3306,//端口号 database : 'nodetest',//数据库名 user : 'root',//数据库用户名 password : '123456'//数据库密码 }; 2. 封装、暴露方法 let mysql = require('mysql');//引入mysql模块 var databaseConfig = require('./mysql.config'); //引入数据库配置模块中的数据 //向外暴露方法 module.exports = { query : function(sql,params,callback){ //每次使用的时候需要创建链接，数据操作完成之后要关闭连接 var connection = mysql.createConnection(databaseConfig); connection.connect(function(err){ if(err){ console.log('数据库链接失败'); throw err; } //开始数据操作 //传入三个参数，第一个参数sql语句，第二个参数sql语句中需要的数据，第三个参数回调函数 connection.query( sql, params, function(err,results,fields ){ if(err){ console.log('数据操作失败'); throw err; } //将查询出来的数据返回给回调函数 callback && callback(results, fields); //results作为数据操作后的结果，fields作为数据库连接的一些字段 //停止链接数据库，必须再查询语句后，要不然一调用这个方法，就直接停止链接，数据操作就会失败 connection.end(function(err){ if(err){ console.log('关闭数据库连接失败！'); throw err; } }); }); }); } }; 3. demo var db=require('../model/mysql.js'); // 查询实例 db.query('select * from t_user', [],function(result,fields){ console.log('查询结果：'); console.log(result); }); //添加实例 var addSql = 'INSERT INTO websites(username,password) VALUES(?,?)'; var addSqlParams =['咕噜先森', '666']; db.query(addSql,addSqlParams,function(result,fields){ console.log('添加成功') }) Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"区块链/bitcoin/":{"url":"区块链/bitcoin/","title":"bitcoin","keywords":"","body":"比特币 Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"区块链/eos/":{"url":"区块链/eos/","title":"eos","keywords":"","body":"EOS Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"区块链/ethereum/":{"url":"区块链/ethereum/","title":"ethereum","keywords":"","body":"以太坊 Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"区块链/其他/":{"url":"区块链/其他/","title":"其他","keywords":"","body":"其他区块链 Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"文档编写/markdown语法.html":{"url":"文档编写/markdown语法.html","title":"markdown语法","keywords":"","body":"转自： .md即markdown文件的基本常用编写语法（图文并茂） 1、标题的几种写法： 第一种： 前面带#号，后面带文字，分别表示h1-h6,上图可以看出，只到h6，而且h1下面会有一条横线，注意，#号后面有空格 第二种： 这种方式好像只能表示一级和二级标题，而且=和-的数量没有限制，只要大于一个就行 第三种： 这里的标题支持h1-h6，为了减少篇幅，我就偷个懒，只写前面二个，这个比较好理解，相当于标签闭合，注意，标题与#号要有空格 那既然3种都可以使用，可不可以混合使用呢？我试了一下，是可以的，但是为了让页面标签的统一性，不建议混合使用，推荐使用第一种，比较简洁，全面 为了搞清楚原理，我特意在网上搜一下在线编写markdown的工具，发现实际上是把这些标签最后转化为html标签，如图： 在线地址请看这里： markdown在线编辑 （只是想看看背后的转换原理，没有广告之嫌） 2、列表 我们都知道，列表分为有序列表和无序列表，下面直接展示2种列表的写法： 可以看到，无序列表可以用 ， + ， — 来创建，用在线编辑器看，实际上是转换成了ul>li ，所以使用哪个都可以，推荐使用吧 有序列表就相对简单一点，只有这一种方式，注意，数字后面的点只能是英文的点，特别注意，有序列表的序号是根据第一行列表的数字顺序来的，比如说： 第一组本来是3 2 1 倒序，但是现实3 4 5 ，后面一组 序号是乱的， 但是还是显示 3 4 5 ，这点必须注意了 列表嵌套 (子列表前面加3个空格) 一级有序列表内容 二级有序列表内容 二级有序列表内容 二级有序列表内容 3、区块引用 比如说，你想对某个部分做的内容做一些说明或者引用某某的话等，可以用这个语句 无序列表下方的便是引用，可以有多种用途，看你的需求了，用法就是在语句前面加一个 > ，注意是英文的那个右尖括号，注意空格 引用因为是一个区块，理论上是应该什么内容都可以放，比如说：标题，列表，引用等等，看看下图： 将上面的代码稍微改一下，全部加上引用标签，就变成了一个大的引用，还有引用里面还有引用，那引用嵌套引用还没有别的写法呢？ 上图可以看出，想要在上一次引用中嵌套一层引用，只需多加一个>，理论上可以无限嵌套，我就不整那么多了，注意：多层嵌套的>是不需要连续在一起的，只要在一行就可以了，中间允许有空格，但是为了好看，还是把排版搞好吧 4、华丽的分割线 分割线可以由* - _（星号，减号，底线）这3个符号的至少3个符号表示，注意至少要3个，且不需要连续，有空格也可以 应该看得懂吧，但是为了代码的排版好看，你们自己定规则吧，前面有用到星号，建议用减号 5、链接 支持2种链接方式：行内式和参数式，不管是哪一种，链接文字都是用 [方括号] 来标记。 上图可知，行内式的链接格式是：链接的文字放在[]中，链接地址放在随后的（）中，举一反三，经常出现的列表链接就应该这样写： 链接还可以带title属性，好像也只能带title，带不了其他属性，注意，是链接地址后面空一格，然后用引号引起来 这是行内式的写法，参数式的怎么写： 这就好理解了，就是把链接当成参数，适合多出使用相同链接的场景，注意参数的对应关系，参数定义时，这3种写法都可以： foo: http://example.com/ 'Optional Title Here' 还支持这种写法，如果你不想混淆的话： 其实还有一种隐式链接的写法，但是我觉得那种写法不直观，所以就不写了，经常用的一般就上面2种，如果你想了解隐式链接，可以看我文章最后放出的参考地址 6、图片 图片也有2种方式：行内式和参数式， 用法跟链接的基本一样，唯一的不同就是，图片前面要写一个！（这是必须的），没什么好说的 7、代码框 这个就比较重要了，很多时候都需要展示出一些代码 如果代码量比较少，只有单行的话，可以用单反引号包起来，如下： 要是多行这个就不行了，多行可以用这个： 多行用三个反引号，如果要写注释，可以在反引号后面写 8、表格 这个写的有点麻烦，注意看 从这3种不同写法看，表格的格式不一定要对的非常起，但是为了好看，对齐肯定是最好的，第一种的分割线后面的冒号表示对齐方式，写在左边表示左对齐，右边为右对齐，两边都写表示居中，还是有点意思的 9、强调 1个星号或者是1个下划线包起来，会转换为 倾斜 如果是2个，会转换为 加粗 如果是3个，就是 倾斜加粗 10、转义 就不一一列举了，基本上跟js转义是一样的 11、删除线 Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"面试经验/python/":{"url":"面试经验/python/","title":"python","keywords":"","body":" Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"面试经验/python/元类metaclass.html":{"url":"面试经验/python/元类metaclass.html","title":"元类metaclass","keywords":"","body":"2、Python中的元类(metaclass) 类也是对象： >>> print ObjectCreator # 你可以打印一个类，因为它其实也是一个对象 >>> def echo(o): … print o … >>> echo(ObjectCreator) # 你可以将类做为参数传给函数 >>> print hasattr(ObjectCreator, 'new_attribute') Fasle >>> ObjectCreator.new_attribute = 'foo' # 你可以为类增加属性 >>> print hasattr(ObjectCreator, 'new_attribute') True >>> print ObjectCreator.new_attribute foo >>> ObjectCreatorMirror = ObjectCreator # 你可以将类赋值给一个变量 >>> print ObjectCreatorMirror() 动态地创建类： a、可以在函数中创建类，使用class关键字即可。 b、type可以接受一个类的描述作为参数，然后返回一个类。 type(类名, 父类的元组（针对继承的情况，可以为空），包含属性的字典（名称和值）) 例如： >>> MyShinyClass = type('MyShinyClass', (), {}) # 返回一个类对象 >>> print MyShinyClass >>> print MyShinyClass() # 创建一个该类的实例 用字典定义属性 + 继承 + 增加方法 >>> Foo = type('Foo', (), {'bar':True}) >>> FooChild = type('FooChild', (Foo,),{}) >>> print FooChild >>> print FooChild.bar # bar属性是由Foo继承而来 True >>> def echo_bar(self): … print self.bar … >>> FooChild = type('FooChild', (Foo,), {'echo_bar': echo_bar}) >>> hasattr(Foo, 'echo_bar') False >>> hasattr(FooChild, 'echo_bar') True >>> my_foo = FooChild() >>> my_foo.echo_bar() True 总结：在Python中，类也是对象，你可以动态的创建类。这就是当你使用关键字class时Python在幕后做的事情，而这就是通过元类来实现的。 元类就是用来创建这些类（对象）的，元类就是类的类。 MyClass = type('MyClass', (), {}) type就是Python在背后用来创建所有类的元类。 任何一个class的class属性是type： >>> class Bar(object): pass >>> b = Bar() >>> b.__class__ >>> b.__class__.__class__ type就是Python的内建元类，当然了，你也可以创建自己的元类。class Foo(object): __metaclass__ = something… […] 你首先写下class Foo(object)，但是类对象Foo还没有在内存中创建。Python会在类的定义中寻找metaclass属性，如果找到了，Python就会用它来创建类Foo。 如果Python没有找到metaclass，它会继续在Bar（父类）中寻找metaclass属性，并尝试做和前面同样的操作。如果Python在任何父类中都找不到metaclass，它就会在模块层次中去寻找metaclass，并尝试做同样的操作。如果还是找不到metaclass,Python就会用内置的type来创建这个类对象。 元类的作用： 1、拦截类的创建 2、修改类 3、返回修改之后的类 例如：构造一个元类，使所有属性都改成大写形式 class UpperAttrMetaclass(type): def __new__(cls, name, bases, dct): attrs = ((name, value) for name, value in dct.items() if not name.startswith('__')) uppercase_attr = dict((name.upper(), value) for name, value in attrs) uppercase_attr['say_' + name] = lambda self, value, saying=name: print(saying + ',' + value + '!') return super(UpperAttrMetaclass, cls).__new__(cls, name, bases, uppercase_attr) # python3 中取消了 __metaclass__属性，只能在定义类的时候声明metaclass参数来创建元类 class MyClass(object, metaclass=UpperAttrMetaclass): bar = 1 print(hasattr(MyClass, 'bar')) # 输出: False print(hasattr(MyClass, 'BAR')) # 输出: True f = MyClass() print(f.BAR) # 输出：1 f.say_MyClass('Hello') # 输出：MyClass,Hello! Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"面试经验/python/迭代器和生成器.html":{"url":"面试经验/python/迭代器和生成器.html","title":"迭代器和生成器","keywords":"","body":"摘自迭代器和生成器 可迭代对象：例如：list、dict 迭代器：所有你可以使用 for .. in .. 语法的叫做一个迭代器：列表，字符串，文件…… 当你有大量数据的时候，是不能使用迭代器的，因为要把所有值存储到内存中。 生成器：生成器是可以迭代的，但是你 只可以读取它一次 ，因为它并不把所有的值放在内存中，它是实时地生成数据: 例如下面代码中，for i in mygenerator只能使用一次，第二次将没有值。>>> mygenerator = (x*x for x in range(3)) >>> for i in mygenerator : ... print(i) 0 1 4 yield关键字：yield 是一个类似 return 的关键字，只是这个函数返回的是个生成器。 当你调用这个函数的时候，函数内部的代码并不立马执行，这个函数只是返回一个生成器对象。 当你使用for进行迭代的时候才执行函数。 每次迭代都会到yield处暂停，并且返回yield后面的值。下次迭代会从yield处开始继续执行。>>> def createGenerator() : ... mylist = range(3) ... for i in mylist : ... yield i*i ... >>> mygenerator = createGenerator() # create a generator >>> print(mygenerator) # mygenerator is an object! >>> for i in mygenerator: ... print(i) 0 1 4 extend() 是一个迭代器方法，作用于迭代器，并把参数追加到迭代器的后面。mygenerator.extend(object)>>> a = [1, 2] >>> b = [3, 4] >>> a.extend(b) >>> print(a) [1, 2, 3, 4] itertools：可以获取全排列、全组合 horses = [1, 2, 3, 4] 其中2个元素的全组合： a = itertools.combinations(horses,2) print(list(a)) # [(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)] 全排列： b = itertools.permutations(horses) print(list(b)) 举例：比赛中4匹马可能到达终点的先后顺序的可能情况 import itertools >>> horses = [1, 2, 3, 4] >>> races = itertools.permutations(horses) >>> print(races) >>> print(list(itertools.permutations(horses))) [(1, 2, 3, 4), (1, 2, 4, 3), (1, 3, 2, 4), (1, 3, 4, 2), (1, 4, 2, 3), (1, 4, 3, 2), (2, 1, 3, 4), (2, 1, 4, 3), (2, 3, 1, 4), (2, 3, 4, 1), (2, 4, 1, 3), (2, 4, 3, 1), (3, 1, 2, 4), (3, 1, 4, 2), (3, 2, 1, 4), (3, 2, 4, 1), (3, 4, 1, 2), (3, 4, 2, 1), (4, 1, 2, 3), (4, 1, 3, 2), (4, 2, 1, 3), (4, 2, 3, 1), (4, 3, 1, 2), (4, 3, 2, 1)] 迭代器的内部机理 迭代是一个实现可迭代对象(实现的是 iter() 方法)和迭代器(实现的是 next() 方法)的过程。可迭代对象是你可以从其获取到一个迭代器的任一对象。迭代器是那些允许你迭代可迭代对象的对象。 面试问题： 问：将列表生成式中[]改成() 之后数据结构是否改变？ 答：是，从列表变为生成器 >>> L = [x*x for x in range(10)] >>> L [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] >>> g = (x*x for x in range(10)) >>> g at 0x0000028F8B774200> 通过列表生成式，可以直接创建一个列表。但是，受到内存限制，列表容量肯定是有限的。而且，创建一个包含百万元素的列表，不仅是占用很大的内存空间，如：我们只需要访问前面的几个元素，后面大部分元素所占的空间都是浪费的。因此，没有必要创建完整的列表（节省大量内存空间）。在Python中，我们可以采用生成器：边循环，边计算的机制—>generator Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"面试经验/python/面向切面编程AOP和装饰器.html":{"url":"面试经验/python/面向切面编程AOP和装饰器.html","title":"面向切面编程AOP和装饰器","keywords":"","body":"AOP 这种在运行时，动态地将代码切入到类的指定方法、指定位置上的编程思想就是面向切面的编程。 面向对象的特点是继承、多态和封装。而封装就要求将功能分散到不同的对象中去，这在软件设计中往往称为职责分配。实际上也就是说，让不同的类设计不同的方法。这样代码就分散到一个个的类中去了。这样做的好处是降低了代码的复杂程度，使类可重用。 但是人们也发现，在分散代码的同时，也增加了代码的重复性。什么意思呢？比如说，我们在两个类中，可能都需要在每个方法中做日志。按面向对象的设计方法，我们就必须在两个类的方法中都加入日志的内容。也许他们是完全相同的，但就是因为面向对象的设计让类与类之间无法联系，而不能将这些重复的代码统一起来。 也许有人会说，那好办啊，我们可以将这段代码写在一个独立的类独立的方法里，然后再在这两个类中调用。但是，这样一来，这两个类跟我们上面提到的独立的类就有耦合了，它的改变会影响这两个类。那么，有没有什么办法，能让我们在需要的时候，随意地加入代码呢？这种在运行时，动态地将代码切入到类的指定方法、指定位置上的编程思想就是面向切面的编程。 一般而言，我们管切入到指定类指定方法的代码片段称为切面，而切入到哪些类、哪些方法则叫切入点。有了AOP，我们就可以把几个类共有的代码，抽取到一个切片中，等到需要时再切入对象中去，从而改变其原有的行为。 这样看来，AOP其实只是OOP的补充而已。OOP从横向上区分出一个个的类来，而AOP则从纵向上向对象中加入特定的代码。有了AOP，OOP变得立体了。如果加上时间维度，AOP使OOP由原来的二维变为三维了，由平面变成立体了。从技术上来说，AOP基本上是通过代理机制实现的。 AOP在编程历史上可以说是里程碑式的，对OOP编程是一种十分有益的补充。 装饰器 http://kissg.me/2016/07/16/translation-about-python-decorator/ https://foofish.net/python-decorator.html 装饰器是一个很著名的设计模式，经常被用于有切面需求的场景，较为经典的有插入日志、性能测试、事务处理等。装饰器是解决这类问题的绝佳设计，有了装饰器，我们就可以抽离出大量函数中与函数功能本身无关的雷同代码并继续重用。 装饰器的作用就是为已经存在的对象添加额外的功能。 装饰器就是”包装纸(wrapper)”, 也就是说, 装饰器允许你在被装饰的函数前后执行代码, 而不对函数本身做任何修改。 装饰器示例 def use_logging(func): def wrapper(): logging.warn(\"%s is running\" % func.__name__) return func() return wrapper def foo(): print(\"i am foo\") a = use_logging(foo) a() 使用@语法糖 def use_logging(func): def wrapper(): logging.warn(\"%s is running\" % func.__name__) return func() return wrapper @use_logging def foo(): print(\"i am foo\") foo() use_logging 就是一个装饰器，它一个普通的函数，它把执行真正业务逻辑的函数 func 包裹在其中，看起来像 foo 被 use_logging 装饰了一样，use_logging 返回的也是一个函数，这个函数的名字叫 wrapper。在这个例子中，函数进入和退出时 ，被称为一个横切面，这种编程方式被称为面向切面的编程。 装饰器可以带参数： 这里wrapper接收的参数就是foo函数的参数，可以任意自定义，这里用args, *kwargs定义了一个通用的装饰器。这里是一个装饰器的装饰器。一个能接收任意参数的装饰器代码片段(decorator)。 然后, 我们用另一个函数(use_logging)来创建我们的装饰器, 以接收参数。 @use_logging(level=\"warn\")等价于@decorator (当然还是会优先执行use_logging的内容) def use_logging(level): def decorator(func): def wrapper(*args, **kwargs): if level == \"warn\": logging.warn(\"%s is running\" % func.__name__) elif level == \"info\": logging.info(\"%s is running\" % func.__name__) return func(*args) return wrapper return decorator @use_logging(level=\"warn\") def foo(name='foo'): print(\"i am %s\" % name) foo() 类装饰器：装饰器不仅可以是函数，还可以是类，相比函数装饰器，类装饰器具有灵活度大、高内聚、封装性等优点。使用类装饰器主要依靠类的call方法，当使用 @ 形式将装饰器附加到函数上时，就会调用此方法。 class Foo(object): def __init__(self, func): self._func = func def __call__(self): print ('class decorator runing') self._func() print ('class decorator ending') @Foo def bar(): print ('bar') bar() '''输出 class decorator runing bar class decorator ending ''' 引入functools模块, 该模块包含的funtools.wraps()函数, 可以将被装饰函数名称, 模块, 文档字符串(docstring)拷贝到它的包装纸中。 wraps本身也是一个装饰器。 如果没有wraps装饰器，这里会输出 'with_logging' 和 None。 from functools import wraps def logged(func): @wraps(func) def with_logging(*args, **kwargs): print func.__name__ # 输出 'f' print func.__doc__ # 输出 'does some math' return func(*args, **kwargs) return with_logging @logged def f(x): \"\"\"does some math\"\"\" return x + x * x 装饰器顺序：一个函数还可以同时定义多个装饰器 @a @b @c def f (): pass # 它的执行顺序是从里到外，最先调用最里层的装饰器，最后调用最外层的装饰器，它等效于 f = a(b(c(f))) 如何使用装饰器 经典的用法是, 拓展一个外部库函数(你无法修改它)的功能, 或者用于调试(因为它是临时的, 你并不想进行修改). 当然, 装饰器的好处是, 对于几乎所有东西, 在不重写的情况, 马上可用. Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"面试经验/python/新式类和经典类.html":{"url":"面试经验/python/新式类和经典类.html","title":"新式类和经典类","keywords":"","body":"新式类和经典类 (python3中只有新式类，在python3中以下两种写法就都表示新式类) http://www.cnblogs.com/btchenguang/archive/2012/09/17/2689146.html 新式类 class C(object): pass 经典类 class B: pass 简单的说，新式类是在创建的时候继承其他类，或者继承内置object对象（或者是从内置类型，如list,dict等），而经典类是直接声明的。 内置的object对象：object对象定义了一系列特殊的方法实现所有对象的默认行为。 使用dir(class)方法可以查看，例如：dir(C) new，init方法 这两个方法是用来创建object的子类对象，静态方法new()用来创建类的实例，然后再调用 init()来初始化实例。 delattr, getattribute, setattr方法 对象使用这些方法来处理属性的访问 hash, repr, str方法 print(someobj)会调用someobj.str()， 如果str没有定义，则会调用someobj.repr()， str()和repr()的区别： repr的目标是对象信息唯一性 str的目标是对象信息的可读性 容器对象的str一般使用的是对象元素的repr 如果重新定义了repr，而没有定义str，则默认调用str时，调用的是repr 好的编程习惯是每一个类都需要重写一个repr方法，用于提供对象的可读信息， 重写str方法是可选的。实现str方法，一般是需要更加好看的打印效果，比如你要制作一个报表的时候等。 可以允许object的子类重载这些方法，或者添加新的方法。 新式类 新式类除了拥有经典类的全部特性之外，还有一些新的特性。比如init发生了变化，新增了静态方法new。 new是构造方法，init是初始化。 新式类都有一个new的静态方法，它的原型是object.new(cls[, ...]) cls是一个类对象，当你调用C(args, **kargs)来创建一个类C的实例时，python的内部调用是C.new(C, args, **kargs)，然后返回值是类C的实例c。 在确认c是C的实例后，python再调用C.init(c, args, *kargs)来初始化实例c。 如果重写了new方法，那么必须调用该类的实例，python才会自动调用init方法来初始化该实例，例如： # 新式类 class Test(object): def __new__(cls, *args, **kwargs): print('__new__') print(args) return super(Test, cls).__new__(cls) def __init__(self,s): print('__init__') print(s) t = Test('hello') '''输出 __new__ ('hello',) __init__ hello ''' 使用new来实现Singleton单例模式 class Singleton(object): _singletons = {} def __new__(cls): if not cls in cls._singletons: #若还没有任何实例 cls._singletons[cls] = object.__new__(cls) #生成一个实例 return cls._singletons[cls] #返回这个实例 a = Singleton() b = Singleton() print(id(a)) print(id(b)) 新式类的实例：除了新式类本身具有新的特性外，新式类的实例也具有新的特性。 Property： 会对属性的访问方式产生影响，调用proprety()是一种创建data descriptor的一种简洁的方式。 x = property(fget=None, fset=None, fdel=None, doc=None) #返回的是property对象，可以赋值给某属性。 fget，fset，fdel 都是实例的方法，例如实例a： a.x 就是调用a.fget() a.x = 1 就是调用a.fset(1) del a.x 就是调用a.fdel() slots属性 通常每一个实例x都会有一个dict属性，用来记录实例中所有的属性和方法，也是通过这个字典，可以让实例绑定任意的属性。 而slots属性作用就是，当类C有比较少的变量，而且拥有slots属性时，类C的实例 就没有dict属性，而是把变量的值存在一个固定的地方。 如果试图访问一个slots中没有的属性，实例就会报错。 这样操作有什么好处呢？slots属性虽然令实例失去了绑定任意属性的便利，但是因为每一个实例没有dict属性，却能有效节省每一个实例的内存消耗，有利于生成小而精干的实例。 怎么去定义slots属性？ class A(object): def __init__(self): self.x = 1 self.y = 2 __slots__ = 'x','y' a = A() dir(a) # 可以看出，实例a中没有 __dict__字典 a.z = 3 # 报错，实例a不能随意添加新的属性，如果不定义__slot__则可以随意添加 注意： 当一个类的父类没有定义slots属性，父类中的dict属性总是可以访问到的，所以只在子类中定义slots属性，而不在父类中定义是没有意义的。 如果定义了slots属性，还是想在之后添加新的变量，就需要把'dict'字符串添加到slots__的元组里。 定义了slots属性，还会消失的一个属性是weakref，这样就不支持实例的weak reference，如果还是想用这个功能，同样，可以把'weakref'字符串添加到元组里。 slots功能是通过descriptor实现的，会为每一个变量创建一个descriptor。 slots的功能只影响定义它的类，因此，子类需要重新定义slots才能有它的功能。 getattribute方法 对新式类的实例来说，所有属性和方法的访问操作都是通过getattribute完成，这是由object基类实现的。如果有特殊的要求，可以重载getattribute方法。 实例的方法 实例的私有属性会覆盖掉类中定义的同名属性。例如类A中有个方法 foo()，实例中可以重新定义自己的foo()方法，使用a.foo = newfoo即可覆盖。 隐式调用实例的私有特殊方法时，新的对象模型和经典对象模型表现上不太一样。 在经典对象模型中，无论是显示调用还是隐式调用特殊方法，都会调用实例中后绑定的特殊方法。 而在新的对象模型中，除非显式地调用实例的特殊方法，否则python总是会去调用类中定义的特殊方法，如果没有定义的话，就报错。 例如： def fakeGetItem(index): return index + 1 Class A(object): pass a = A() a.__getitem__ = fakeGetItem print(a.__getitem__) # 显示调用会正常输出2 print(a[1]) # 调用a[1]，将产生一个隐式的__getitem__方法的调用，在新式类中，因为类中没有定义这个方法，也不是object基类有的方法，所以报错。需要显示地调用才可以运行。 新的对象模型 新式类能够从python的内置类型中继承，而经典类不行。 多继承：新式类同样支持多继承，但是如果新式类想要从多个内置类型中继承生成一个新类的话，则这些内置类必须是经过精心设计，能够互相兼容的。显然，python也没会让你随意的从多个内置类中进行多继承，想创建一个超级类不是那么容易的。。。通常情况下，至多可以继承一个内置类，比如list, set, dict等。 MRO (Method Resolution Order，方法解析顺序) 举例：B和C继承D，A继承B和C A / \\ B C \\ / D a是A的实例，现在a要使用属性m。 在经典类中，方法和属性的查找链是从左到右，深度优先的方式进行查找，所以这里的查找顺序是：A->B->D->C->A，假如 D和C都定义了属性m，这里会直接找到D的m值并直接获取了，会绕过C中覆盖的m值，但实际C继承了D，应该返回C的m值，这是一个bug，在新式类中修复。 新式类采用的是从左到右，广度有限定方式进行查找，所以查找循序为 A->B->C->D，可以返回类C的属性m。 这个顺序的实现是通过新式类中特殊的只读属性mro，类型是一个元组，保存着解析顺序信息。只能通过类来使用，不能通过实例调用。 class D(object): def __init__(self): self.m = 1 class B(D): pass class C(D): def __init__(self): self.m = 2 # 因为是从左到右的广度优先，这里如果是写C,B，那么则是C在前 class A(B,C): pass a = A() print(a.m) # 输出2 print(A.__mro__) # (, , , , ) 协作式调用父类方法 class D(object): def foo(self): print(\"D's foo\") class B(D): def foo(self): print(\"B's foo\") # D.foo(self) super(B, self).foo() class C(D): def foo(self): print(\"C's foo\") # D.foo(self) super(C, self).foo() class A(B,C): def foo(self): print(\"A's foo\") # B.foo(self) # C.foo(self) super(A, self).foo() a = A() a.foo() '''输出 # 如果是直接调用父类方法，会输出两次 D's foo，结果为： A's foo B's foo D's foo C's foo D's foo # 如果是使用super方法，可以返回superobject(超对象，不是简单的父类的对象)，使用超对象调用父类方法时，可以保证只被运行一次，结果为： A's foo B's foo C's foo D's foo ''' 如果养成了使用super去调用父类方法的习惯，那么你的类就可以适应无论多么复杂的继承调用结构。 super()可以看成是更加安全调用父类方法的一种新方式。 Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"面试经验/python/单例模式.html":{"url":"面试经验/python/单例模式.html","title":"单例模式","keywords":"","body":"单例模式 单例模式是一种常用的软件设计模式。在它的核心结构中只包含一个被称为单例类的特殊类。 通过单例模式可以保证系统中一个类只有一个实例而且该实例易于外界访问，从而方便对实例个数的控制并节约系统资源。 如果希望在系统中某个类的对象只能存在一个，单例模式是最好的解决方案。 new()在init()之前被调用，用于生成实例对象。 单例模式是指创建唯一对象，单例模式设计的类只能有一个实例。 使用new方法 (cls有一个_instance属性，可以判断是否已经有实例) class Singleton(object): def __new__(cls, *args, **kw): if not hasattr(cls, '_instance'): orig = super(Singleton, cls) cls._instance = orig.__new__(cls, *args, **kw) return cls._instance class MyClass(Singleton): a = 1 a = Singleton() b = Singleton() c = MyClass() d = MyClass() print(id(a)) print(id(b)) print(id(c)) print(id(d)) ''' 4个输出都一样，如果去掉 Singleton的__new__方法，则4个输出都不一样 ''' 共享属性 创建实例时把所有实例的dict指向同一个字典,这样它们具有相同的属性和方法. class Borg(object): _state = {} def __new__(cls, *args, **kw): ob = super(Borg, cls).__new__(cls, *args, **kw) ob.__dict__ = cls._state return ob class MyClass2(Borg): a = 1 a = MyClass2() b = MyClass2() a.m = 1 b.n = 2 print(a.__dict__) # {'m': 1, 'n': 2} print(b.__dict__) # {'m': 1, 'n': 2} 装饰器版本 def singleton(cls): instances = {} def getinstance(*args, **kw): if cls not in instances: instances[cls] = cls(*args, **kw) return instances[cls] return getinstance @singleton class MyClass: a = 1 a = MyClass() b = MyClass() print(id(a)) # 3028452353136 print(id(b)) # 3028452353136 import方法：作为python的模块是天然的单例模式 # mysingleton.py class My_Singleton(object): def foo(self): pass my_singleton = My_Singleton() # to use from mysingleton import my_singleton my_singleton.foo() Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"面试经验/python/Python的List的实现.html":{"url":"面试经验/python/Python的List的实现.html","title":"Python的List的实现","keywords":"","body":"Python的List的实现 https://www.jianshu.com/p/J4U6rR C实现 list申请内存空间的大小(allocated) 通常大于 list实际存储元素所占空间的大小(ob_size)，也可能等于，小于时才会重新申请新的内存空间。这是为了避免每次有新元素加入list时都要调用realloc进行内存分配。 我们想要在list中追加一个整数:L.append(1)，假如新的长度大于已申请的内存，会调用list_resize()，list_resize()会申请多余的空间以避免调用多次list_resize()函数，list增长的模型是:0, 4, 8, 16, 25, 35, 46, 58, 72, 88, … 当弹出list的最后一个元素：L.pop()时。调用listpop()，如果这时ob_size(新的list长度)小于allocated（已经申请的内存空间）的一半，list_resize在函数listpop()内部被调用。这时申请的内存空间将会缩小。 我们能看到 Python 设计者的苦心。在需要的时候扩容,但又不允许过度的浪费,适当的内存回收是非常必要的。 这个确定调整后的空间大小算法很有意思，调整后的空间肯定能存储 newsize 个元素。要关注的是预留空间的增长状况。 当新元素数量 (newsize) 小于已申请内存空间大小 (allocated) 的一半时，需要调整申请的内存空间大小 调整后大小 (new_allocated) = 新元素数量 (newsize) + 预留空间 (new_allocated) 预留算法:(newsize // 8) + (newsize 当 newsize >= allocated,自然按照这个新的长度 \"扩容\" 内存。 而如果 newsize 引自《深入Python编程》 Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"面试经验/python/Python2和3的区别.html":{"url":"面试经验/python/Python2和3的区别.html","title":"Python2和3的区别","keywords":"","body":"Python2和3的区别 https://chenqx.github.io/2014/11/10/Key-differences-between-Python-2-7-x-and-Python-3-x/ future模块 Python 3.x 介绍的 一些Python 2 不兼容的关键字和特性可以通过在 Python 2 的内置 future 模块导入。 例如，如果我想要 在Python 2 中表现 Python 3.x 中的整除，我们可以通过如下导入： from __future__ import division 可以导入的模块有： nested_scopes、generators、division、absolute_import、with_statement、print_function、unicode_literals print函数 Python 2 的 print 声明已经被 Python 3中的 print() 函数取代了。 print 在 Python 2 中是一个声明，而不是一个函数调用。 整除 当2个整数相除时，/ 在 python2 和 python3 中有区别： python2是整除，结果是int python3不是整除，结果是float。 # Python2 print '3 / 2 =', 3 / 2 # 1 print '3 // 2 =', 3 // 2 # 1 print '3 / 2.0 =', 3 / 2.0 # 1.5 print '3 // 2.0 =', 3 // 2.0 # 1.0 # Python3 print('4 / 2 =', 4 / 2) # 2.0 print('3 / 2 =', 3 / 2) # 1.5 print('3 // 2 =', 3 // 2) # 1 print('3 / 2.0 =', 3 / 2.0) # 1.5 print('3 // 2.0 =', 3 // 2.0) # 1.0 Unicode python2 默认编码是系统默认编码(如linux是utf-8，windows是gbk)，并且有单独的 unicode类型。 python3 默认编码就是unicode，unicode编码会被直接认为是字符串类型。 Python3有两种表示字符序列的类型：bytes和str。前者的实例包含原始的8位值，后者的实例包含Unicode字符。 Python2也有两种表示字符序列的类型，分别叫做str和Unicode。与Python3不同的是，str实例包含原始的8位值；而unicode的实例，则包含Unicode字符。 编写Python程序的时候，一定要把编码和解码操作放在界面最外围来做。程序的核心部分应该使用Unicode字符类型（也就是Python3中的str、Python2中的unicode）。 # python 3 print(type('\\u03BC\\u0394')) # print(type(b' bytes for storing data')) # ，这个在python2中是str类型 xrange模块 在 Python 2 中 xrange() 创建迭代对象的用法是非常流行的。比如： for 循环或者是列表/集合/字典推导式。 因为直接创建一个迭代器，不用生成list，所以比range()执行更快。 python3中去除了range()，直接将xrange()重命名为range()。 但实际上 Python 3 的 range() 比 Python 2 的 xrange() 运行慢。Python 3 倾向于比 Python 2 运行的慢一点。 # python2 x = xrange(1,3) print type(x) # x = range(1,3) print type(x) # # python3 x = range(1,3) print(type(x)) # Python3中的range对象的contains方法 Python 3 中 range 有一个新的 contains 方法，contains 方法可以加速 “查找” 在 Python 3.x 中显著的整数和布尔类型。 Raising exceptions Python 2 接受新旧两种语法标记，在 Python 3 中如果我不用小括号把异常参数括起来就会阻塞（并且反过来引发一个语法异常）。 # python2，以下两种写法都支持 raise IOError, \"file error\" raise IOError(\"file error\") # python3，只支持一种写法 raise IOError(\"file error\") Handling exceptions 在 Python 3 中处理异常也轻微的改变了，在 Python 3 中我们现在使用 as 作为关键词。 # python2 try: let_us_cause_a_NameError except NameError, err: print err, '--> our error message' # python3 try: let_us_cause_a_NameError except NameError as err: print(err, '--> our error message') next()函数 和 .next()方法 python3 删除了.next()方法 # python3 my_generator = (letter for letter in 'abcdefg') next(my_generator) # a For循环变量和全局命名空间泄漏 在 Python 3.x 中 for 循环变量不会再导致命名空间泄漏。 # python 2 中使用列表推导会导致循环控制变量泄露进周围的作用范围域 print 'Python', python_version() i = 1 print 'before: i =', i # before: i = 1 print 'comprehension: ', [i for i in range(5)] # comprehension: [0, 1, 2, 3, 4] print 'after: i =', i # after: i = 4 列表推导不再支持 [... for var in item1, item2, ...] 这样的语法。使用 [... for var in (item1, item2, ...)] 代替 # python 3 print([i for i in range(5)]) # [0, 1, 2, 3, 4] print([i for i in [1,2,3]]) # [1, 2, 3] print([i for i in (1,2,3)]) # [1, 2, 3] 比较不可排序类型 在 Python 3 中的另外一个变化就是当对不可排序类型做比较的时候，会抛出一个类型错误。 而python 2中比较 [1,2] 和 'foo'的大小不会报错。 通过input()解析用户的输入 在 Python 3 中已经解决了把用户的输入存储为一个 str 对象的问题。 为了避免在 Python 2 中的读取非字符串类型的危险行为，我们不得不使用 raw_input() 代替。 在python2中使用input()读取数字，会存储为int类型，如果使用raw_input()读取数字，会存储为str类型。 返回可迭代对象，而不是列表 假如只遍历一次，我们用可迭代对象可以节约内存。 如果需遍历多次，可以用list()函数把迭代对象转换成一个列表。 # python2 print range(3) # [0, 1, 2] print type(range(3)) # # python3 print(range(3)) # range(0, 3) print(type(range(3))) # print(list(range(3))) # [0, 1, 2] 在 Python 3 中一些经常使用到的不再返回列表的函数和方法： zip() map() filter() dictionary’s .keys() method dictionary’s .values() method dictionary’s .items() method Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"面试经验/操作系统/":{"url":"面试经验/操作系统/","title":"操作系统","keywords":"","body":" Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"面试经验/操作系统/Linux的IO多路复用select,poll,epoll.html":{"url":"面试经验/操作系统/Linux的IO多路复用select,poll,epoll.html","title":"Linux的IO多路复用select,poll,epoll","keywords":"","body":"Linux:IO多路复用select,poll,epoll https://www.cnblogs.com/Anker/p/3265058.html 《如果这篇文章说不清epoll的本质，那就过来掐死我吧》 https://zhuanlan.zhihu.com/p/63179839 https://zhuanlan.zhihu.com/p/64138532 https://zhuanlan.zhihu.com/p/64746509 select，poll，epoll都是IO多路复用的机制。I/O多路复用就通过一种机制，可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。 select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的。 异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。 select 基本原理： select函数监视的文件描述符分为3类，分别为write_fds、read_fds、except_fds。 调用select函数后，进程会阻塞，直到描述符就绪（有数据 可读、可写、或者有except），或者超时(timeout指定等待时间，如果立即返回设为null即可)，函数返回。 当select函数返回后，可以通过遍历fd_set，来找到就绪的描述符。 调用过程： 调用select函数，进程阻塞，然后使用copy_from_user从用户空间拷贝fd_set到内核空间 (fd是文件描述符，或者是socket句柄) 注册回调函数__pollwait 遍历所有fd，调用其对应的poll方法（对于socket，这个poll方法是sock_poll，sock_poll根据情况会调用到tcp_poll,udp_poll或者datagram_poll） 以tcp_poll为例，其核心实现就是__pollwait，也就是上面注册的回调函数。 __pollwait的主要工作就是把current（当前进程）挂到设备的等待队列中，不同的设备有不同的等待队列，对于tcp_poll来说，其等待队列是sk->sk_sleep（注意把进程挂到等待队列中并不代表进程已经睡眠了）。在设备收到一条消息（网络设备）或填写完文件数据（磁盘设备）后，会唤醒设备等待队列上睡眠的进程，这时current便被唤醒了。 poll方法返回时会返回一个描述读写操作是否就绪的mask掩码，根据这个mask掩码给fd_set赋值。 （接3）如果遍历完所有的fd，还没有返回一个可读写的mask掩码，则会调用schedule_timeout使调用select的进程（也就是current）进入睡眠。当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的进程。如果超过一定的超时时间（schedule_timeout指定），还是没人唤醒，则调用select的进程会重新被唤醒获得CPU，进而重新遍历fd，判断有没有就绪的fd。 把fd_set从内核空间拷贝到用户空间。 总结select的几大缺点： 每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大 (上面步骤1) 同时每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大 (上面步骤3-6) select支持的文件描述符数量太小了，默认是1024 poll poll的实现和select非常相似，只是描述fd集合的方式不同，poll使用pollfd结构而不是select的fd_set结构，突破了select中描述符数目的限制，其他的都差不多。 epoll select和poll都只提供了一个函数——select或者poll函数。而epoll提供了三个函数，epoll_create,epoll_ctl和epoll_wait。 epoll_create是创建一个epoll句柄；epoll_ctl是注册要监听的事件类型；epoll_wait则是等待事件的产生。 对于第一个缺点，epoll的解决方案在epoll_ctl函数中。每次注册新的事件到epoll句柄中时（在epoll_ctl中指定EPOLL_CTL_ADD），会把所有的fd拷贝进内核，而不是在epoll_wait的时候重复拷贝。epoll保证了每个fd在整个过程中只会拷贝一次。 对于第二个缺点，epoll的解决方案不像select或poll一样每次都把current轮流加入fd对应的设备等待队列中，而只在epoll_ctl时把current挂一遍（这一遍必不可少）并为每个fd指定一个回调函数，当设备就绪，唤醒等待队列上的等待者时，就会调用这个回调函数，而这个回调函数会把就绪的fd加入一个就绪链表）。epoll_wait的工作实际上就是在这个就绪链表中查看有没有就绪的fd（利用schedule_timeout()实现睡一会，判断一会的效果，和select实现中的第7步是类似的）。 对于第三个缺点，epoll没有这个限制，它所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于2048,举个例子,在1GB内存的机器上大约是10万左右，具体数目可以cat /proc/sys/fs/file-max察看,一般来说这个数目和系统内存关系很大。 我的理解： select的调用过程： 假设现在有一个进程A，A的代码中定义了3个fd，分别是 fd1、fd2、fd3。 然后A进程会调用select()函数，操作系统就会把A从工作队列中移除，加入到3个fd各自的等待队列中。表现为A被阻塞，不会往下执行代码，也不会占用CPU资源。 任何一个fd就绪或者超时(就绪：有数据 可读、可写、或者有except，超时：超过等待时间），中断程序将唤起进程A。 当进程A被唤醒后，它知道至少有一个fd接收了数据(或者超时)。程序只需遍历一遍fd列表，就可以得到就绪的fd。 select的缺点： 每次调用select都要讲进程加入到所有fd的等待队列，每次唤醒又都要从各个等待队列移除。而且每次都要将整个fds列表传递给内核，有一定的开销。 正是因为遍历操作开销大，出于效率的考量，才会规定select的最大监视数量，默认只能监视1024个fd。 进程被唤醒后，程序并不知道哪些fd收到数据，还需要遍历一次。 epoll的调用过程： 当某个进程调用epoll_create方法时，内核会创建一个eventpoll对象。eventpoll对象含有一个 就绪队列rdlist。 还是假设有3个fd，通过epoll_ctl方法添加fd1、fd2、fd3的监视时，内核会将eventpoll添加到这三个fd的等待队列中。 当fd收到数据后，中断程序会给eventpoll的 就绪队列 添加fd引用。 eventpoll对象相当于是fd和进程之间的中介，fd的数据接收并不直接影响进程，而是通过改变eventpoll的就绪列表来改变进程状态。 当程序执行到epoll_wait时，如果rdlist已经引用了fd，那么epoll_wait直接返回，如果rdlist为空，阻塞进程。 eventpoll同样有一个等待队列，进程A的程序执行到epoll_wait时，如果rdlist为空，会将进程A放入eventpoll的等待队列中，阻塞进程。 当fd接收到数据时，中断程序一方面修改rdlist，另一方面唤醒eventpoll等待队列中的进程，进程A再次进入运行状态。 也是因为rdlist的存在，进程A可以知道哪些fd发生了变化。 程序可能随时调用epoll_ctl添加监视fd，也可能随时删除。当删除时，若该fd已经存放在就绪列表中，它也应该被移除。 所以就绪列表应是一种能够快速插入和删除的数据结构。双向链表就是这样一种数据结构，epoll使用双向链表来实现就绪队列（对应上图的rdllist）。 epoll的改进点： select调用之后每次收到数据，都会返回，下次再调用select()又要把所有的fd从用户态发到内核态。 而epoll只用 调用epoll_create创建eventpoll对象 和 调用epoll_ctl添加fd监听，之后只需要在合适的时机调用epoll_wait即可，这样可以获得rdlist中所有就绪的fd了。 一方面减少了fd多次传递给内核的开销，因为会由rdlist来保存就绪的fd，而不用返回给进程。 另一方面减少了进程遍历fd列表的开销，因此进程可以直接从rdlist中得知哪些fd是就绪状态。(select是进程知道某个fd就绪了，但不知道具体是哪一个，只能遍历一次) epoll没有最大支持文件描述符数量的限制。它所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于2048,举个例子,在1GB内存的机器上大约是10万左右，具体数目可以cat /proc/sys/fs/file-max察看,一般来说这个数目和系统内存关系很大。 Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"面试经验/操作系统/调度算法.html":{"url":"面试经验/操作系统/调度算法.html","title":"调度算法","keywords":"","body":"调度算法：根据系统的资源分配策略所规定的资源分配算法 先来先服务(FCFS, First Come First Serve) 既可用于作业调度， 也可用于进程调度。 有利于长作业（进程），而不利于短作业（进程）。 适合于CPU繁忙型作业， 而不利于I/O繁忙型的作业（进程）。 短作业优先(SJF, Shortest Job First) 既可用于作业调度， 也可用于进程调度。 但其对长作业不利。 不能保证紧迫性作业（进程）被及时处理。 作业的长短只是被估算出来的。 最高优先权调度(Priority Scheduling) 为了照顾紧迫性作业，使之进入系统后便获得优先处理。 常被用在批处理系统中，作为作业调度算法，也作为多种操作系统中的进程调度，还可以用于实时系统中。 当其用于作业调度， 将后备队列中若干个优先权最高的作业装入内存。 当其用于进程调度时，把处理机分配给就绪队列中优先权最高的进程，此时，可以将算法分为2种类型： 非抢占式优先权算法 抢占式优先权调度算法（高性能计算机操作系统） 动态优先权：高响应比优先调度算法。 作业的优先等级随着等待时间的增加而以速率a提高。 优先权=（等待时间+要求服务时间）/要求服务时间；即 =（响应时间）/要求服务时间 时间片轮转(RR, Round Robin) 一般用于进程调度。 执行过程： 每次调度，把CPU分配队首进程，并令其执行一个时间片。 当执行的时间片用完时，由一个记时器发出一个时钟中断请求，该进程被停止，并被送往就绪队列末尾；依次循环。 多级反馈队列调度(multilevel feedback queue scheduling) 也是基于时间片的调度算法。 不必事先知道各种进程所需要执行的时间。 执行过程： 设置多个就绪队列，并为各个队列赋予不同的优先级。在优先权越高的队列中， 为每个进程所规定的执行时间片就越小。 一个新进程进入内存后，首先放入第一队列的末尾，按FCFS原则排队等候调度。 如果他能在一个时间片中完成，便可撤离；如果未完成，就转入第二队列的末尾，在同样等待调度…… 如此下去，当一个长作业（进程）从第一队列依次将到第n队列（最后队列）后，便按第n队列时间片轮转运行。 仅当第一队列空闲时，调度程序才调度第二队列中的进程运行； 仅当第1到第（ i-1 ）队列空时， 才会调度第i队列中的进程运行，并执行相应的时间片轮转。 如果处理机正在处理第i队列中某进程，又有新进程进入优先权较高的队列， 则此新队列抢占正在运行的处理机，并把正在运行的进程放在第i队列的队尾。 Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"面试经验/操作系统/死锁.html":{"url":"面试经验/操作系统/死锁.html","title":"死锁","keywords":"","body":"死锁 原因 竞争资源 通常系统中拥有的不可剥夺资源，其数量不足以满足多个进程运行的需要，相似的进程在运行过程中会因争夺资源而陷入僵局。 只有对不可剥夺资源的竞争才可能产生死锁，对可剥夺资源的竞争是不会引起死锁的。 程序推进顺序不当 进程在运行过程中，请求和释放资源的顺序不当，同样会导致死锁。 信号量使用不当也会造成死锁。进程间相互等待对方发来的消息，结果也会造成某些进程间无法继续向前推进。 必要条件 互斥条件：资源不能被共享，只能由一个进程使用。 请求和保持条件：已经得到资源的进程可以再次申请新的资源。 不剥夺条件：已经分配的资源不能从相应的进程中被强制地剥夺。 循环等待条件：系统中若干进程组成环路，该环路中每个进程都在等待相邻进程正占用的资源。 处理死锁基本方法: 预防死锁(破坏除1以外的必要条件，因为无法允许系统资源共享使用) (破坏条件3)当一个以保持了某些不可剥夺资源的进程，请求新的资源时得不到满足，它必须释放已经保持的所有资源，待以后需要时再重新申请。这种方法常用于状态易于保存和恢复的资源，如CPU的寄存器及内存资源，一般不能用于打印机之类的资源。 (破坏条件2)采用预先静态分配方法， 即进程在运行前一次申请完他所需要的全部资源，在他的资源未满足前，不把它投入运行。一旦运行后，这些资源就一直归它所有，也不再提出其他资源请求，这样就可以保证系统不会发生死锁。 这种方式实现简单，但缺点也显而易见，系统资源被严重浪费，其中有些资源可能仅在运行初期或末期才使用，甚至根本不使用。而且还会导致“饥饿”现象，当由于个别资源长期被个别资源占用时，将只是等待该资源的进程迟迟不能开始运行。 (破坏条件4)采用顺序资源分配法。首先给系统中的资源编号，规定每个进程，必须按编号递增的顺序请求资源，同类资源一次申请完。也就是说，只要进程提出申请分配资源，则该进程在以后的资源申请中，只能申请编号比之前大的资源。 避免死锁(银行家算法) 把操作系统看作是银行家，操作系统管理的资源相当于银行家管理的资金，进程向操作系统请求分配资源相当于用户向银行家贷款。 当进程首次申请资源时，要测试该进程对资源的最大需求量，如果系统现存的资源可以满足他的最大需求量，则按当前的申请量分配资源，否则就推迟分配。 当进程在执行中继续申请资源时，先测试该进程已占用的资源数与本次申请的资源数之和是否超过了该进程对资源的最大需求量。 若超过，则拒绝分配资源，若没有超过则在测试系统现存的资源能否满足该进程尚需的最大资源量，若能满足则按当前的申请量分配资源，否则也要推迟分配。 检测死锁(资源分配图) 用圆圈代表一个进程，用方框代表一类资源。 从进程到资源的有向边叫请求边，表示该进程请申请一个单位的该类资源；从资源到进程的边叫做分配边，表示该类资源已经有一个资源被分配到了该进程。 可以通过将资源分配图简化的方法来检测系统状态S是否为死锁状态。 资源分配图简化方法： 在资源分配图中，找出既不阻塞又不是孤点的进程Pi，Pi存在有向边，且Pi申请的所有资源均小于系统中已有的空闲资源数量。这可以消去Pi的所有请求边和分配边，使之成为孤立的节点。 进程Pi所释放的资源，可以唤醒某些因等待这些资源而阻塞的进程，原来的阻塞进程可能变为非阻塞进程。然后继续用1的方式消除。 S为死锁的条件是当且仅当S状态的资源分配图是不可简化的，该条件为死锁定理。 解除死锁 剥夺资源：挂起某些思索进程，并抢占它的资源，将这些资源分配给其他的死锁进程。但应防止被挂起的进程长时间得不到资源时，而处于资源匮乏的状态。 撤销进程：强制撤销一个或一部分进程并剥夺这些进程的资源。撤销的原则可以按进程的优先级和撤销进程代价的高低进行。 进程回退(一般不用，系统开销极大)：让一个或多个进程回退到足以回避死锁的地步，进程回退时资源释放资源而不是被剥夺。要求系统保持进程的历史信息，设置还原点。 Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"面试经验/操作系统/页面置换算法.html":{"url":"面试经验/操作系统/页面置换算法.html","title":"页面置换算法","keywords":"","body":"页面置换算法 进程从内存中获取某一页的时候，如果内存中没有即是缺页，需要调用页面置换算法用目标页将某个页面置换出来。为了减小缺页率，需要选择更优秀的算法 最佳置换算法OPT：选择那些不再使用或者最长时间不再使用的页。 由于无法预知内存中哪些页不再使用，或者最长时间不再使用，因此该算法是无法实现的。 先进先出FIFO：总是淘汰最先进入内存的页（即在内存时间中待得最久）。 最近最久未使用算法LRU:最近一段时间里最久没有使用过的页面予以置换. clock算法（也称NRU算法，最近未使用算法，近似于LRU算法）： 该算法在页表中设置访问字段A和修改位M，当页面被访问(读或写)时设置访问位A为1，当修改后的页面被写入时设置修改位M为1。 所以，A和M有四种可能： A = 0,M = 0.最近未被访问，也没有被修改（是最佳的置换页面） A = 0,M = 1.最近未被访问，但是被修改过 A = 1,M = 0.最近被访问过，但是未被修改 A = 1,M = 1.最近被访问过，也被修改过（所以该页很可能再次被访问） 用A位和M位来构造一个简单的页面置换算法： 当启动一个进程时，它的所有页面的两个位都由操作系统设置成0，A位被定期地（比如在每次时钟中断时）清零，以区别最近没有被访问的页面和被访问的页面。 具体做法： 将内存中的所有页面用指针链接成一个循环队列 从指针所指的位置，扫描循环队列，找的是 A = 0,M = 0的页，遇到便淘汰。第一次扫描期间不改变其访问位。 如果第一次扫描没有找到，那么找 A = 0,M = 1的页面，遇到则淘汰，第二次扫描期间，所经历的页，访问位修改为0。 重复 2和3，一定能找到 A = 0,M = 0 或者 A = 0，M = 1的页面 Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"面试经验/操作系统/epoll的边沿触发模式ET和水平触发模式LT.html":{"url":"面试经验/操作系统/epoll的边沿触发模式ET和水平触发模式LT.html","title":"epoll的边沿触发模式ET和水平触发模式LT","keywords":"","body":"epoll:边沿触发模式(ET)和水平触发模式(LT) 水平触发(level-trggered) 只要文件描述符关联的读内核缓冲区非空，有数据可以读取，就一直发出可读信号进行通知， 当文件描述符关联的内核写缓冲区不满，有空间可以写入，就一直发出可写信号进行通知 LT模式支持阻塞和非阻塞两种方式。epoll默认的模式是LT。 边缘触发(edge-triggered) 当文件描述符关联的内核读缓冲区由空转化为非空的时候，则发出可读信号进行通知， 当文件描述符关联的内核写缓冲区由满转化为不满的时候，则发出可写信号进行通知 两者的区别在哪里呢？水平触发是只要读缓冲区有数据，就会一直触发可读信号，而边缘触发仅仅在空变为非空的时候通知一次。 LT(level triggered)是缺省的工作方式，并且同时支持block和no-block socket。 在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的fd进行IO操作。 如果你不做任何操作，内核还是会继续通知你的，所以，这种模式编程出错误可能性要小一点。传统的select/poll都是这种模型的代表． 水平触发和边缘触发模式区别 读缓冲区刚开始是空的，现在往读缓冲区写入2KB数据。 水平触发和边缘触发模式此时都会发出可读信号。 收到信号通知后，读取了1kb的数据，读缓冲区还剩余1KB数据。 水平触发会再次进行通知，而边缘触发不会再进行通知。 所以，边缘触发需要一次性的把缓冲区的数据读完为止，也就是一直读，直到读到EGAIN为止，EGAIN说明缓冲区已经空了，因为这一点，边缘触发需要设置文件句柄为非阻塞。 Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"面试经验/数据库/":{"url":"面试经验/数据库/","title":"数据库","keywords":"","body":" Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"面试经验/数据库/数据库索引.html":{"url":"面试经验/数据库/数据库索引.html","title":"数据库索引","keywords":"","body":"数据库索引 MySQL支持诸多存储引擎，而各种存储引擎对索引的支持也各不相同，因此MySQL数据库支持多种索引类型，如BTree索引，哈希索引，全文索引等等。 通常使用最多的是BTree索引。 http://blog.codinglabs.org/articles/theory-of-mysql-index.html https://zhuanlan.zhihu.com/p/27700617 平衡二叉树 非叶子节点最多拥有两个子节点，叶子节点没有子节点 左孩子的值 数据的左右两边的节点层级相差不会大于1，避免树形结构由于删除增加变成线性链表影响查询效率，保证数据平衡的情况下查找数据的速度近于二分法查找 没有值相等重复的节点 B树(B-tree，就读做B树，并不是B减树) 和 平衡二叉树 稍有不同的是，B树属于多叉树，又名\"平衡多路查找树\"(查找路径不只两个)。 规则： 排序方式：所有节点关键字是按递增次序排列，并遵循左小右大原则 子节点数：非叶节点的子节点数>1，且=2 (注：M阶代表一个树节点最多有多少个查找路径，M=M路,当M=2则是2叉树,M=3则是3叉) 关键字数：枝节点的关键字数量大于等于ceil(M/2)-1个且小于等于M-1个（ceil是向上取整，关键字数=指针数-1，指针数对应子节点数，最大是M，所以关键字数最大是M-1) 所有叶子节点均在同一层、叶子节点除了包含了关键字和关键字记录的指针外也有指向其子节点的指针只不过其指针地址都为null 为什么非根非叶节点的关键字数量大于等于 ceil(m/2)-1？ 因为关键字数小于 ceil(m/2)-1 的非根非叶节点在平衡过程中势必可以合并消除。 假设存在一个节点A，它的关键字数小于 ceil(m/2)-1 情况一：其邻近兄弟节点C关键字数大于 ceil(m/2)-1，则可以向C借一个关键字转化为他俩的新父节点，原父节点下移到A节点的最右边。 例如，这是一个5阶平衡查找树，A节点只有一个关键字，CDE节点有3个关键字(大于2)，那么可以转换。 B C / \\ => / \\ A CDE AB DE 情况二：其邻近兄弟节点C关键字数等于 ceil(m/2)-1，此时不能再向兄弟借。则将父节点中位于兄弟俩中间的数下移到A节点的最右边，并且与兄弟节点合并。 例如： B E E / \\ \\ => / \\ A CD FG ABCD FG 情况三：其邻近兄弟节点C关键字数小于 ceil(m/2)-1，则可以合并或消除节点。 例如： B D D / \\ \\ => / \\ A C EF ABC EF * m阶B树的定义如下： 1. 根节点至少包括两个孩子 2. 树中每个节点最多含有m个孩子（m>=2） 3. 除根节点和叶节点外，其他每个节点至少有ceil(m/2)个孩子（ceil()为向上取整） 4. 所有叶子节点都位于同一层 5. 每个非叶节点包括n个关键字信息，其中ceil(m/2)-14就要进行节点拆分），同时关键字数 >= cell(5/2)-1 = 2 2. 排序规则：满足节点本身比左边节点大，比右边节点小的排序规则 构建步骤： 1. 节点的关键字数要小于等于4，所以先插入 3、8、31、11 3 8 11 31 2. 插入23，因为节点的关键字数要大于等于2，所以拆开成3个节点，再然后插入29 和 50 11 11 / \\ => / \\ 3 8 23 31 3 8 23 29 31 50 3. 再插入28，右子节点关键字数又达到5个，需要拆开 11 29 / / \\ 3 8 23 28 31 50 * B树节点的删除 遵循规则： 1. 节点合并规则：当前是要组成一个5路查找树，那么此时m=5,关键字数必须大于等于 ceil(5/2)-1 = 2(关键字数 / \\ 3 8 11 23 31 50 3 8 23 29 31 50 * B树的特点 + B树相对于平衡二叉树的不同是，每个节点包含的关键字增多了 + 特别是在B树应用到数据库中的时候，数据库充分利用了磁盘块的原理（磁盘数据存储是采用块的形式存储的，每个块的大小为4K，每次IO进行数据读取时，同一个磁盘块的数据可以一次性读取出来）把节点大小限制和充分使用在磁盘快大小范围 + 把树的节点关键字增多后树的层级比原来的二叉树少了，减少数据查找的次数和复杂度 B+树 B+树是B树的一个升级版，相对于B树来说B+树更充分的利用了节点的空间，让查询速度更加稳定，其速度完全接近于二分法查找，查找的效率要比B树更高、更稳定。 规则 B+跟B树不同的是：B+树的非叶子节点不保存关键字记录的指针，只进行数据索引，这样使得B+树每个非叶子节点所能保存的关键字大大增加 B+树叶子节点保存了父节点的所有关键字记录的指针，所有数据地址必须要到叶子节点才能获取到。所以每次数据查询的次数都一样 B+树叶子节点的关键字从小到大有序排列，左边结尾数据都会保存右边节点开始数据的指针 非叶子节点的子节点数=关键字数 (mysql的B+树采用的是这种方式，实际上有另外一种实现，就是 非叶节点的关键字数=子节点数-1) 特点 B+树的层级更少：相较于B树B+每个非叶子节点存储的关键字数更多，树的层级更少所以查询数据更快 B+树查询速度更稳定：B+所有关键字数据地址都存在叶子节点上，所以每次查找的次数都相同所以查询速度要比B树更稳定 B+树天然具备排序功能：B+树所有的叶子节点数据构成了一个有序链表，在查询大小区间的数据时候更方便，数据紧密性很高，缓存的命中率也会比B树高 B+树全节点遍历更快：B+树遍历整棵树只需要遍历所有的叶子节点即可，而不需要像B树一样需要对每一层进行遍历，这有利于数据库做全表扫描 B树相对于B+树的优点: 如果经常访问的数据离根节点很近，而B树的非叶子节点本身存有关键字其数据的地址，所以这种数据检索的时候会要比B+树快。 B-/+Tree索引的性能分析 磁盘预读 首先，由于磁盘读写慢，因此每次都会预读。 这样做的理论依据是计算机科学中著名的局部性原理：当一个数据被用到时，其附近的数据也通常会马上被使用。 预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。 B-/+Tree索引的优势 假设B树高度为h，可知检索一次最多需要访问h个节点。数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。 每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。 B-Tree中一次检索最多需要h-1次I/O（根节点常驻内存），渐进复杂度为O(h)=O(logdN)。一般实际应用中，出度d是非常大的数字，通常超过100，因此h非常小（通常不超过3）。 而红黑树这种结构，h明显要深的多。由于逻辑上很近的节点（父子）物理上可能很远，无法利用局部性，所以红黑树的I/O渐进复杂度也为O(h)，效率明显比B-Tree差很多。 MySQL索引实现 MyISAM索引实现 MyISAM引擎使用B+Tree作为索引结构，叶节点的data域存放的是数据记录的地址。 MyISAM的索引方式也叫做“非聚集”的，之所以这么称呼是为了与InnoDB的聚集索引区分。 InnoDB索引实现 InnoDB也使用B+Tree作为索引结构，但具体实现方式却与MyISAM截然不同。 区别： InnoDB的数据文件本身就是索引文件。 MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。 而在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶节点data域保存了完整的数据记录(包括其他非索引列的数据)。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。 这种索引叫做 \"聚集索引\"。 因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有）。 如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形。 假设某张表 table_user有3列，uid、name、age，索引列是uid。 在MyISAM中，叶节点的data域记录的是 数据记录的地址，然后依据地址去找到该行其他列的数据。 而在InnoDB中，假设uid是主键，那么叶节点的data域记录的是 name和age。 InnoDB的辅助索引data域存储相应记录主键的值而不是地址。即InnoDB的所有辅助索引都引用主键作为data域。 假如现在索引列是age，那么InnoDB的叶节点存的是其对应的主键uid的值。 因此，辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录。 最左前缀原理与相关优化 (B+树) 假设有一个组合索引 (uid,name,age) 只用uid做条件，或者用 uid+name做条件，也是可以命中索引的，但是只能用到一部分索引。 假如用 uid+age做条件，和只用uid没有区别。 匹配某列的前缀字符串也可以命中索引(uid+name的索引)。 例如 uid = 1001 and name like '%zhang' 范围查询可以用到索引，但必须是最左前缀。 如果指定uid和name,加上age的范围，则可以命中完整的索引。(uid+name+age)。 例如 uid 注意：between不一定是范围匹配，如果作用在这里的uid上，则是精确匹配，相当于 in。 如果查询条件中有函数或者表达式，则无法命中索引。 例如： uid + 1 = 1001 可以加前缀索引 假如现在要查两个字段 firstname + lastname，但是2个字段加起来很长，我们可以对lastname加前缀索引。 ALTER TABLE employees.employees ADD INDEX first_name_last_name4 (first_name, last_name(4)); 执行之后，可以通过 show profiles;来查看运行的性能(时间)。 InnoDB的主键选择与插入优化 InnoDB使用聚集索引，数据记录本身被存于主索引（一颗B+Tree）的叶子节点上。 这就要求同一个叶子节点内（大小为一个内存页或磁盘页）的各条数据记录按主键顺序存放，因此每当有一条新的记录插入时，MySQL会根据其主键将其插入适当的节点和位置。 如果页面达到装载因子（InnoDB默认为15/16），则开辟一个新的页（节点）。 在使用InnoDB存储引擎时，如果没有特别的需要，请永远使用一个与业务无关的自增字段作为主键。 如果表使用自增主键，那么每次插入新的记录，记录就会顺序添加到当前索引节点的后续位置。这样就会形成一个紧凑的索引结构，近似顺序填满。 如果使用非自增主键（如果身份证号或学号等），由于每次插入主键的值近似于随机，因此每次新纪录都要被插到现有索引页得中间某个位置。 此时MySQL不得不为了将新记录插到合适位置而移动数据，甚至目标页面可能已经被回写到磁盘上而从缓存中清掉，此时又要从磁盘上读回来，这增加了很多开销。 同时频繁的移动、分页操作造成了大量的碎片，得到了不够紧凑的索引结构，后续不得不通过OPTIMIZE TABLE来重建表并优化填充页面。 因此，只要可以，尽量在InnoDB上采用自增字段做主键。 Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"面试经验/数据库/乐观锁和悲观锁.html":{"url":"面试经验/数据库/乐观锁和悲观锁.html","title":"乐观锁和悲观锁","keywords":"","body":"乐观锁和悲观锁 悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作 总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）。 传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。 Java中synchronized和ReentrantLock等独占锁就是悲观锁思想的实现。 乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。 总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。 乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于write_condition机制，其实都是提供的乐观锁。 在Java中java.util.concurrent.atomic包下面的原子变量类就是使用了乐观锁的一种实现方式CAS实现的。 CAS(Compare And Set)：在提交的时候对比变量的值和原值，发生改变则不操作。CAS是由硬件来保证原子性的，但是只能保证单个变量，涉及多个变量就无能为力了。 版本号机制：给数据加上一个version字段，更新时带上version条件。例如：update A set A.id = 10 where version='xxx';如果查询是针对表A，更新是针对表2，版本号机制就无法处理了。 CAS的缺点： 1. 其他线程将数据更新后改回原值，CAS是无感知的，称为ABA问题。 2. 并发冲突概率大的高竞争环境下，CAS一直失败，会一直重试，CPU开销大。可以引入退出机制，在超过一定阈值后失败退出。但最好还是避免在高竞争环境使用乐观锁。 3. CAS只能保证单个变量操作的原子性，涉及多个变量这无能为力 4. CAS的实现需要硬件的支持，普通用户无法直接使用，灵活度受限。 两种锁的使用场景 乐观锁适用于写比较少的情况下（多读场景）。 一般多写的场景下用悲观锁就比较合适。 Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"面试经验/数据库/MVCC及事务隔离性.html":{"url":"面试经验/数据库/MVCC及事务隔离性.html","title":"MVCC及事务隔离性","keywords":"","body":"MVCC (在事务操作的数据快照上进行，并不修改实际的数据，提交时才修改) 全称是Multi-Version Concurrent Control，即多版本并发控制。 在MVCC协议下，每个读操作会看到一个一致性的snapshot，并且可以实现非阻塞的读。 MVCC允许数据具有多个版本，这个版本可以是时间戳或者是全局递增的事务ID，在同一个时间点，不同的事务看到的数据是不同的。 例如：a=1，A事务中更新了a=2，但还没有提交，此时A事务读到的A是2，B事务读到的A是1。 MySQL的innodb引擎是如何实现MVCC的 innodb会为每一行添加两个字段，分别表示该行创建的版本和删除的版本，填入的是事务的版本号，这个版本号随着事务的创建不断递增。 在repeated read的隔离级别（事务的隔离级别请看这篇文章）下，具体各种数据库操作的实现： select：满足以下两个条件innodb会返回该行数据： 该行的创建版本号小于等于当前版本号，用于保证在select操作之前所有的操作已经执行落地。 该行的删除版本号大于当前版本或者为空。删除版本号大于当前版本意味着有一个并发事务将该行删除了。 insert：将新插入的行的创建版本号设置为当前系统的版本号。 delete：将要删除的行的删除版本号设置为当前系统的版本号。 update：不执行原地update，而是转换成insert + delete。将旧行的删除版本号设置为当前版本号，并将新行insert同时设置创建版本号为当前版本号。 其中，写操作（insert、delete和update）执行时，需要将系统版本号递增。 由于旧数据并不真正的删除，所以必须对这些数据进行清理，innodb会开启一个后台线程执行清理工作，具体的规则是将删除版本号小于当前系统版本的行删除，这个过程叫做purge。 通过MVCC很好的实现了事务的隔离性，可以达到repeated read级别，要实现serializable还必须加锁。 隔离性 在事务并发操作时，可能出现的问题有： 脏读：还是上面的例子，假如B事务读到的是A事务还没有提交的2，然后A又提交失败了，B读到的就是脏数据。 不可重复读：在同一个事务中，对于同一份数据读取到的结果不一致。假如B事务在A提交前读到的是1，A提交后读到的是2，就是不可重复读。 幻读：在同一个事务中，同一个查询多次返回的结果不一致。假如A添加了一条记录 b=3，B在A提交前只读到一行数据，在A提交后读到两行数据，这个就是幻读。 事务的隔离级别从低到高有： Read Uncommitted(读取未提交)：最低的隔离级别，什么都不需要做，一个事务可以读到另一个事务未提交的结果。所有的并发事务问题都会发生。 Read Committed(读取已提交)：只有在事务提交后，其更新结果才会被其他事务看见。可以解决脏读问题。 Repeated Read（可重读）：在一个事务中，对于同一份数据的读取结果总是相同的，无论是否有其他事务对这份数据进行操作，以及这个事务是否提交。可以解决脏读、不可重复读。但幻读仍有可能发生。 Serialization(可串行化)：事务串行化执行，隔离级别最高，牺牲了系统的并发性。可以解决并发事务的所有问题。 Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"面试经验/网络/":{"url":"面试经验/网络/","title":"网络","keywords":"","body":" Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"面试经验/网络/三次握手和四次挥手.html":{"url":"面试经验/网络/三次握手和四次挥手.html","title":"三次握手和四次挥手","keywords":"","body":"三次握手 (发起TCP连接) 客户端通过向服务器端发送一个SYN来创建一个主动打开，作为三次握手的一部分。客户端把这段连接的序列号设定为随机数 A，称为ISN。 服务器端应当为一个合法的SYN回送一个SYN/ACK。ACK 的确认码应为 A+1，SYN/ACK 包本身又有一个随机序号 B。 最后，客户端再发送一个ACK，确认码为B+1。当服务端收到这个ACK的时候，就完成了三路握手，并进入了连接创建状态。 初始序号 ISN 当新连接建立的时候，握手报文的第一个字节数据的序号称为 ISN(Initial Sequence Number)，即初始序号。 ISN的意义是 可以让接收方生成一个合法的接收窗口。 ISN是一个32位动态随机数，一是为了避免数据乱序(使用相同的序号会导致数据乱序)，二是避免报文被伪造。 TCP包的类型 (SYN, FIN, ACK, PSH, RST, URG) 在TCP层，有个FLAGS字段，这个字段有以下几个标识：SYN, FIN, ACK, PSH, RST, URG。 对于我们日常的分析有用的就是前面的五个字段。 SYN: 同步序号，用于建立连接过程，在连接请求中，SYN=1和ACK=0表示该数据段没有使用捎带的确认域，而连接应答捎带一个确认，即SYN=1和ACK=1。 FIN: finish标志，用于释放连接，为1时表示发送方已经没有数据发送了，即关闭本方数据流。 ACK: 确认序号标志，为1时表示确认号有效，为0表示报文中不含确认信息，忽略确认号字段。 PSH: push标志，为1表示是带有push标志的数据，指示接收方在接收到该报文段以后，应尽快将这个报文段交给应用程序，而不是在缓冲区排队。 RST: 重置连接标志，用于重置由于主机崩溃或其他原因而出现错误的连接。或者用于拒绝非法的报文段和拒绝连接请求。 URG: 紧急指针标志，为1时表示紧急指针有效，为0则忽略紧急指针。 为什么需要进行三次握手？ 为了保证服务端能收接受到客户端的信息并能做出正确的应答而进行前两次(第一次和第二次)握手 为了保证客户端能够接收到服务端的信息并能做出正确的应答而进行后两次(第二次和第三次)握手。 总结就是，服务端要确认客户端能收到自己的信息才会建立连接，所以需要第三次握手。 如果第三次握手服务端没有收到，则不会建立连接。 如果第三次握手的ACK丢失怎么办？ 如果第三次握手的ACK在网络中丢失，那么Server端该TCP连接的状态为SYN_RECV，并且依次等待3秒、6秒、12秒后重新发送SYN+ACK包，以便Client重新发送ACK包。 Server重发SYN+ACK包的次数，可以通过设置/proc/sys/net/ipv4/tcp_synack_retries修改，默认值为5。 如果重发指定次数后，仍然未收到ACK应答，那么一段时间后，Server自动关闭这个连接。 但是Client认为这个连接已经建立，如果Client端向Server写数据，Server端将以RST包(用于强制关闭tcp连接)响应，方能感知到Server的错误。 四次挥手 (关闭TCP连接) 注意: 中断连接端可以是客户端，也可以是服务器端. 下面仅以客户端断开连接举例, 反之亦然. 1. 客户端发送一个数据分段, 其中的 FIN 标记设置为1（最后一个发送的数据分段中附带FIN标志位）。 客户端进入 FIN-WAIT 状态. 该状态下客户端只接收数据, 不再发送数据. 2. 服务器接收到带有 FIN = 1 的数据分段, 发送带有 ACK = 1 的剩余数据分段, 确认收到客户端发来的 FIN 信息（在某个返回的数据分段中附带ACK标志位，只是确认收到FIN消息）. 3. 服务器等到所有数据传输结束, 向客户端发送一个带有 FIN = 1 的数据分段（最后一个返回的数据分段中附带FIN标志位）, 并进入 CLOSE-WAIT 状态, 等待客户端发来带有 ACK = 1 的确认报文. 4. 客户端收到服务器发来带有 FIN = 1 的报文, 返回 ACK = 1 的报文确认, 为了防止服务器端未收到需要重发, 进入 TIME-WAIT 状态. 服务器接收到报文后关闭连接. 客户端等待 2MSL 后未收到回复, 则认为服务器成功关闭, 客户端关闭连接. 关闭连接和建立连接时的区别 相较于tcp的建立，tcp关闭的时候需要释放两端的资源(发送未完成的数据)，因此多了一个对端的FIN的状态。 关闭TCP连接时一定要四次挥手吗？第二次的ACK和第三次的FIN能否合并？ 不一定要四次挥手。 第二次和第三次完全可以合并，即服务端在收到客户端的FIN报文后，在最后一个数据分段中同时发送 ACK和FIN。 这种方式叫做\"延迟确认优化\"，在RFC793 3.5节。 为什么会设计第二次挥手？ 我的想法： 服务端在收到客户端的FIN报文时，可能仍有一批数据没有传完，无法立即关闭SOCKET，所以只能先回复一个ACK报文。 此时，如果服务端不立即回复一个ACK报文，客户端将无法得知服务端有没有收到自己的FIN报文。于是，客户端就需要定时重发FIN报文。 服务端先发一个ACK报文，客户端就会进入 FIN-WAIT-2状态 (此前是FIN-WAIT-1状态)，等待接收服务端的FIN报文即可。 关闭连接至少需要几次挥手？ 3次。服务端需要确认客户端收到了自己发送的最后一个数据分段(即带有FIN标记的数据分段)，因此至少需要三次挥手。 服务端运行一段时间后，套接字出现了大量的Close_Wait状态，最有可能是什么原因导致的？ 收到了客户端发送的断开请求(FIN报文)，但是服务器还有未发完的数据。 为什么基于TCP的程序往往都有个应用层的心跳检测机制？ TCP建立链接之后，只是在两端的内核里面维持tcp信息，实际上并没有一个物理的连接的通路，对端这个时候挂了，谁也不知道。 TCP四次分手中，主动关闭方最后为什么要等待2MSL之后才关闭连接？ MSL是最大报文生存周期。 假如是客户端主动发起关闭连接，第四次挥手中客户端发送了ACK报文，但是不确定服务端是否有收到。 如果服务端没有收到，则会再次重复第三次挥手(发送FIN报文)，客户端再收到FIN报文，就再重发ACK报文。 如果客户端在2MSL时间内，没有再次收到服务端发来的FIN报文，那么大概率服务端已经收到了ACK报文，客户端可以安心关闭了。 总结：主动关闭方最后等待2MSL后关闭连接，是为了尽量保证被动方收到了ACK。(依然不能完全保证) Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"面试经验/如何写出一份专业的技术简历.html":{"url":"面试经验/如何写出一份专业的技术简历.html","title":"如何写出一份专业的技术简历","keywords":"","body":"转自: 如何写出一份专业的技术简历 在这一部分，我们聊一聊怎么写简历，尤其是技术简历。我们将从简历的本质、简历的内容和简历模版等方面来和大家分享。 简历的本质 首先，在写之前，要明白简历的本质，也就是说，它的作用是什么、写简历是为了什么？ 往深了说，简历其实就是一个销售文案，是你向招聘方销售你的价值主张的载体。你要在里边清楚的告诉招聘方，我是谁、我想来这家公司担任什么职位、我将在这个职位上完成哪些工作、为公司带来怎样的价值、我期望的薪水是多少，赶紧雇用我吧！ 但是这些话也可以留到面试时去讲，所以往浅点说呢，简历最重要的作用就是帮你约到面试。 简历的形态 从形式上来讲，简历并不一定非得是文本的，它可以就是一段视频、一个开源项目、一张照片、甚至是一行字。但对于绝大部分普通人来讲，简历就是一份最好不超过一页的文本。因为就算你不走寻常路，拿到了 offer ，最后办理各种入职手续的时候，HR 还是需要一份文本简历来归档。 所以呢，无论你要准备视频简历还是其他，那都是锦上添花的事，文本简历是逃不掉的，不要想偷懒，好好准备吧。 从格式上来讲，我们推荐使用 Markdown 写，用 PDF 发。使用 Markdown 书写可以更好的控制格式，而通过 PDF 发送可以保证对方的阅读效果，不会因为在不同的机器、不同的操作系统导致样式丢失。 如果你没有称手的简历写作工具，可以试试冷熊简历，它是一个运行在浏览器里边的 Markdown 编辑器，可以一键生成 PDF ，提供简单大方的单页式排版。 简历和面试关系 我一般把简历看成面试的一个提纲，简历上浓缩了你的价值主张，而面试，则是展开讨论这个价值主张的时候。 很多同学觉得面试是招聘方选候选人，是单向的；但实际上，面试也是你选招聘方的时候，在认真回答了招聘方的问题之后，你完全可以围绕着「我如何为公司提供价值」这个核心，提出自己有疑惑的问题。如果招聘方不能很好做出回答，那么很可能他们对于这个岗位、甚至这个部门（尤其是新部门）没有想清楚，失败的可能性会陡增，需要慎重考虑。 简历的内容 很多同学都很喜欢写流水账，我在什么地方出生、上的什么小学中学大学、读过什么专业、得过什么奖、参加过哪些项目、甚至自己的兴趣爱好都写上。 其实简历并不是履历，你要明白你其实是处在一个竞争环境，你是在 推销 自己。你不但要让对方明白你做过什么，还要低调的把你的价值主张潜移默化的植入到招聘方的大脑里边，让他/她不由自主的觉得，「这个人对我们公司非常有价值，我一定不能放过」。 那么如何才能做到呢？三个要点，第一是按 FAB 法则来陈述、第二是给论据但不给论点、第三是提供充足的证明。 FAB 法则 FAB 法则的思路其实非常简单，就是你不但要说「是什么」、还要回答「好在哪里」以及「能给对方带来什么价值」。 下边是一个简单的 FAB 的例子： 2006年，参与了手机XX网发布系统WAPCMS的开发（F，是什么）。 作为核心程序员，不但完成了网站界面、调度队列的开发工作，更提出了高效的组件级缓存系统，通过碎片化缓冲有效的提升了系统的渲染效率（F，细化具体工作，要写出你在这个项目中具体负责的部分，以及你贡献出来的价值）。 在该系统上线后，Web前端性能从10QPS提升到200QPS，服务器由10台减少到3台（A，通过量化的数字进行对比）。 2008年升任WAPCMS项目负责人，带领一个3人小组支持着每天超过2亿的PV（B，你能带给前雇主的价值，也就是你能带给新雇主的价值）。 给论据不给论点 议论文的结构通常是最有说服力的，但是简历是一篇你自己介绍自己的文章，如果你的论点是你很优秀，论据是你这里优秀那里优秀，即使你真的那么优秀，读起来也容易招人反感。因为我们的文化是一个内敛的文化，所以要学会留一些话不要说。 这里的一个小技巧就是，把论点隐藏起来，只大量去说论据。让简历的读者自己去得出这个论点，这样他/她就会相信这是自己的想法，从而更容易接受它。 提供充足的证明 企业招人其实也是有风险的，光是时间成本就是非常大，所以招聘方往往希望从你的简历上寻找一些证明，来降低风险，说服自己招聘这个候选人。 经历证明 最靠谱的证明是「经历证明」，「经历证明」的逻辑是，这个人既然在别的公司做过这样的事情，那么让他/她在我们工作也做这样的事情，应该也能胜任。「经历证明」屡试不爽，即使偶有失误，公司也不会怪罪到招聘方上。经常有同学问我大学毕业应该去大公司还是创业公司，从「经历证明」的角度来看，肯定是去大公司，因为之后你在大公司的这一段「经历证明」，可以帮你很多，甚至在创业时都会很有用。 能力证明 其次就是「能力证明」，「能力证明」的问题在于对招聘方的要求较高，他们得有验证的能力和手段。技术面试就是一种能力证明，开源项目则是另外一种能力证明。在简历中透露一些解决高难度问题的细节，同样是一种能力证明。 当「经历证明」不够强时，要通过「能力证明」来做补充。 学习能力也是一种能力 对于应届生和新人来说，「学习能力」也是一种「能力证明」，它表现出来的就是「成长」。当你的绝对数据不是那么好的时候，要强调增长数据。因为企业对新人的要求，往往并不是有现成的技能（有当然最好，但一般候选人都没有），而是「聪明」和「学得快」。 简历模板 最后我们提供了一系列的简历模板供大家参考，可以访问 GitHub 获取。 Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"测试工具/jmeter压力测试.html":{"url":"测试工具/jmeter压力测试.html","title":"jmeter压力测试","keywords":"","body":" Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"版本控制/git常用命令.html":{"url":"版本控制/git常用命令.html","title":"git常用命令","keywords":"","body":"git常用命令 clone代码，会自动创建目录 git clone git@github.com:Michaelromic/MyGitBook.git ./GitBook 从远程获取最新版本并merge到本地 git pull origin gh-pages 把修改提交到暂存区 git add . 提交说明 (一定要先提交说明) git commit -m \"修正xxxbug\" 推送本地master分支到远端master分支 git push origin master:master 切换 gh-pages 分支 git checkout -b gh-pages 新建 gh-pages 分支: git push origin gh-pages 切换回master分支 git checkout master 查看远程分支 git branch -a 查看本地分支 git branch clone其他分支代码 git clone -b branchname git@github.com:Michaelromic/MyGitBook.git ./branchname git切换ssh和http协议：(也可以直接改当前git目录里面有个配置文件) 查看当前remote: git remote -v 切换到ssh： (远程仓库的名称一般默认为 origin) git remote set-url origin git@github.com:drchen126/b0.git Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"设计与架构/RESTful.html":{"url":"设计与架构/RESTful.html","title":"RESTful","keywords":"","body":"转自： 通俗易懂RESTful，如何设计RESTful风格API 1. 概念 REST -- REpresentational State Transfer 直译：表现层状态转移。这个中文直译经常出现在很多文章中。尼玛，谁听得懂“表现层状态转移”，这是人话吗？ 那就逐个单词来理解REST名称 REST -- REpresentational State Transfer 首先，之所以晦涩是因为前面主语被去掉了，全称是 Resource Representational State Transfer：通俗来讲就是：资源在网络中以某种表现形式进行状态转移。分解开来： Resource：资源，即数据（前面说过网络的核心）。比如 newsfeed，friends等； Representational：某种表现形式，比如用JSON，XML，JPEG等； State Transfer：状态变化。通过HTTP动词实现。 2. 为什么要用RESTful结构呢？ 大家都知道\"古代\"网页是前端后端融在一起的，比如之前的PHP，JSP等。在之前的桌面时代问题不大，但是近年来移动互联网的发展，各种类型的Client层出不穷，RESTful可以通过一套统一的接口为 Web，iOS和Android提供服务。另外对于广大平台来说，比如Facebook platform，微博开放平台，微信公共平台等，它们不需要有显式的前端，只需要一套提供服务的接口，于是RESTful更是它们最好的选择。 a、从原理角度来分析： 根据Richardson Maturity Model（理查德森成熟度模型）, REST架构的成熟度有4个等级： Level 0 - 面向前台 我们在咖啡店向前台点了一杯拿铁，这个过程可以用这段文字来描述： { \"addOrder\": { \"orderName\": \"latte\" } } 我们通过这段文字，告诉前台，新增一笔订单，订单是一杯拿铁咖啡，接着，前台给我们返回这么一串回复： { \"orderId\": \"123456\" } 假设我们有一张会员卡，我们想查询一下这张会员卡的余额，这时候，要向前台发起另一个询问： { \"queryBalance\": { \"cardId\": \"447031335\" } } 查询卡号为447031335的卡的余额，查询的结果返回来了： { \"balance\": \"0\" } 没钱…… 哈哈，没钱，现在我们要跟前台说，这杯咖啡不要了： { \"deleteOrder\": { \"orderId\": \"123456\" } } Level 1 - 面向资源 现在这家咖啡店越做越大，来喝咖啡的人越来越多，单靠前台显然是不行的，店主决定进行分工，每个资源都有专人负责，我们可以直接面向资源操作。 比如还是下单，请求的内容不变，但是我们多了一条消息: /orders { \"addOrder\": { \"orderName\": \"latte\" } } 多了一个斜杠和orders，这是什么意思？ 这个表示我们这个请求是发给哪个资源的，订单是一种资源，我们可以理解为是咖啡厅专门管理订单的人，他可以帮我们处理所有有关订单的操作，包括新增订单、修改订单、取消订单等操作。 接着还是会返回订单的编号给我们： { \"orderId\": \"123456\" } 下面，我们还是要查询会员卡余额，这次请求的资源变成了cards： /cards { \"queryBalance\": { \"cardId\": \"447031335\" } } 接下来是取消订单： /orders { \"deleteOrder\": { \"orderId\": \"123456\" } } Level2 - 打上标签 接下来，店主还想继续优化他的咖啡厅的服务流程，他发现负责处理订单的员工，每次都要去订单内容里面看是新增订单还是删除订单，还是其他的什么操作，十分不方便，于是规定，所有新增资源的请求，都在请求上面写上大大的‘POST’，表示这是一笔新增资源的请求。 其他种类的请求，比如查询类的，用‘GET’表示，删除类的，用‘DELETE’表示，修改用PATCH表示。 来，我们再来重复上面那个过程，来一杯拿铁： POST /orders { \"orderName\": \"latte\" } 请求的内容简洁多啦，不用告诉店员是addOrder，看到POST就知道是新增，返回的内容还是一样： { \"orderId\": \"123456\" } 接着是查询会员卡余额，这次也简化了很多： GET /cards { \"cardId\": \"447031335\" } 这个请求我们还可以进一步优化为这样： GET /cards/447031335 直接把要查询的卡号写在后面了。 没错，接着，取消订单： DELETE /orders/123456 Level 3 - 完美服务 忽然有一天，有个顾客抱怨说，他买了咖啡后，不知道要怎么取消订单，咖啡厅一个店员回了一句，你不会看我们的宣传单吗，上面不写着： DELETE /orders/{orderId} 顾客反问道，谁会去看那个啊，店员不服，又说到，你瞎了啊你……后面两人吵着吵着还打了起来… 噗，真是悲剧… 有了这次教训，店长决定，顾客下了单之后，不仅给他们返回订单的编号，还给顾客返回所有可以对这个订单做的操作，比如告诉用户如何删除订单。现在，我们还是发出请求，请求内容和上一次一样： POST /orders { \"orderName\": \"latte\" } 但是这次返回时多了些内容： { \"orderId\": \"123456\", \"link\": { \"rel\": \"cancel\", \"url\": \"/order/123456\" } } 这次返回时多了一项link信息，里面包含了一个rel属性和url属性，rel是relationship的意思，这里的关系是cancel，url则告诉你如何执行这个cancel操作，接着你就可以这样子来取消订单啦： DELETE /orders/123456 哈哈，这服务真是贴心，以后再也不用担心店员和顾客打起来了。 Level3的Restful API，给使用者带来了很大的遍历，使用者只需要知道如何获取资源的入口，之后的每个URI都可以通过请求获得，无法获得就说明无法执行那个请求。 现在绝大多数的RESTful接口都做到了Level2的层次，做到Level3的比较少。当然，这个模型并不是一种规范，只是用来理解Restful的工具。所以，做到了Level2，也就是面向资源和使用Http动词，就已经很Restful了。 Levels的意义 Level 1 解释了如何通过分治法(Divide and Conquer)来处理复杂问题，将一个大型的服务端点(Service Endpoint)分解成多个资源。 Level 2 引入了一套标准的动词，用来以相同的方式应对类似的场景，移除不要的变化。 Level 3 引入了可发现性(Discoverability)，它可以使协议拥有自我描述(Self-documenting)的能力。 这一模型帮助我们思考我们想要提供的HTTP服务是何种类型的，同时也勾勒出人们和它进行交互时的期望。 b、从应用角度来分析： 一、REST描述的是在网络中client和server的一种交互形式；REST本身不实用，实用的是如何设计 RESTful API（REST风格的网络接口）； 二、Server提供的RESTful API中，URL中只使用名词来指定资源，原则上不使用动词。“资源”是REST架构或者说整个网络处理的核心。 URL定位资源，用HTTP动词（GET,POST,DELETE,DETC）描述操作。 1、看Url就知道要什么 2、看http method就知道干什么 3、看http status code就知道结果如何 比如： http://api.qc.com/v1/newsfeed: 获取某人的新鲜; http://api.qc.com/v1/friends: 获取某人的好友列表; http://api.qc.com/v1/profile: 获取某人的详细信息; 三、用HTTP协议里的动词来实现资源的添加，修改，删除等操作。即通过HTTP动词来实现资源的状态扭转： GET 用来获取资源， POST 用来新建资源（也可以用于更新资源）， PUT 用来更新资源， DELETE 用来删除资源。 比如： DELETE http://api.qc.com/v1/friends: 删除某人的好友 （在http parameter指定好友id） POST http://api.qc.com/v1/friends: 添加好友 UPDATE http://api.qc.com/v1/profile: 更新个人资料 四、Server和Client之间传递某资源的一个表现形式，比如用JSON，XML传输文本，或者用JPG，WebP传输图片等。当然还可以压缩HTTP传输时的数据（on-wire data compression）。 五、用 HTTP Status Code传递Server的状态信息。比如最常用的 200 表示成功，500 表示Server内部错误等。 3. 总结 好了，理解了RESTful的概念，究竟如何应用，这是个问题。根据项目的需求不同，我们的API设计规范也存在差别，完全看自身理解，满足自身需求，大的理念不变，根据需求制定项目的API规范就是好的RESTful。 Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"python/python爬虫/":{"url":"python/python爬虫/","title":"python爬虫","keywords":"","body":" Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"python/python爬虫/编码+文件打开方式+文件命名方式.html":{"url":"python/python爬虫/编码+文件打开方式+文件命名方式.html","title":"编码+文件打开方式+文件命名方式","keywords":"","body":"编码 response.content 返回的是 bytes 型的二进制数据，是文件原生的编码 response.text 返回的是一个字符串，是已经将原生的编码，通过正确的encode转码成unicode了 在python3里面，unicode编码的就是字符串，其他类型编码的都是二进制，即bytes类型 文件打开方式 r只读, r+读写, 文件不存在则异常 w只写, w+读写, 文件不存在则创建 a只写追加, a+读写追加, 文件不存在则创建 文件命名 rfind是从右往左找，这里是取最后一个/后面的子串 url[url.rfind('/')+1:] find是从左往右找，这里是取左侧第一个双斜线后面的子串 start_pos = url.find('//')+2 --取整个url的md5 + 文件后缀 hashlib.md5(url.encode('utf8')).hexdigest() + url[url.rfind('.'):] 上下文管理器 with open(\"sina_b.html\", \"wb+\") as f: c = f.read() 原理如下： class CNDTextIOWrapper(): def __init__(self): print('__init__') pass def __enter__(self): print('__enter__') return self def __exit__(self, type, value, trace): print(\"type:\", type) print(\"value:\", value) print(\"trace:\", trace) def do_something(self): print('do_something') def demo(): return CNDTextIOWrapper() with demo() as d: d.do_something() 执行顺序是 init -> enter -> do_something -> exit enter返回的self就是with这里获取的d对象 Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"python/使用pandas库将excel中多个sheet拆分为单个文件.html":{"url":"python/使用pandas库将excel中多个sheet拆分为单个文件.html","title":"使用pandas库将excel中多个sheet拆分为单个文件","keywords":"","body":"使用pandas库将excel中多个sheet拆分为单个文件 # -*- coding: utf-8 -*- import pandas as pd import os source_file = 'D:/mmaww/source.xlsx' target_dir = 'D:/mmaww/targetdir/' d_read = pd.read_excel(source_file,None) # 获取源文件中每个sheet的名字 names=list(d_read.keys()) # 创建保存目录 # shutil.rmtree 用于删除非空文件夹，os.removedirs仅可删除空文件夹 if os.path.exists(target_dir): shutil.rmtree(target_dir) os.mkdir(target_dir) os.chdir(target_dir) for name in names: print(name) # 读取的时候要写 header=None，否则会默认将第0行作为标题，而标题在输出的时候会加粗加框 (源文件没有加粗加框) tempsheet=pd.read_excel(source_file, sheet_name = name,header=None) savefile = target_dir + name + \".xlsx\" tempsheet.to_excel(savefile, sheet_name = name, na_rep='', index = False, encoding='utf-8', header=None) Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"python/Django.html":{"url":"python/Django.html","title":"Django","keywords":"","body":"安装Django pip install Django 验证 若要验证 Django 是否能被 Python 识别，可以在 shell 中输入 python。 然后在 Python 提示符下，尝试导入 Django： >>> import django >>> print(django.get_version()) 2.1 或者 python -m django --version 创建项目 如果这是你第一次使用 Django 的话，你需要一些初始化设置。也就是说，你需要用一些自动生成的代码配置一个 Django project —— 即一个 Django 项目实例需要的设置项集合，包括数据库配置、Django 配置和应用程序配置。 打开命令行，cd 到一个你想放置你代码的目录，然后运行以下命令： django-admin startproject mysite 创建了如下目录： mysite/ manage.py mysite/ __init__.py settings.py urls.py wsgi.py 这些目录和文件的用处是： 最外层的:file: mysite/ 根目录只是你项目的容器， Django 不关心它的名字，你可以将它重命名为任何你喜欢的名字。 manage.py: 一个让你用各种方式管理 Django 项目的命令行工具。你可以阅读 django-admin and manage.py 获取所有 manage.py 的细节。 里面一层的 mysite/ 目录包含你的项目，它是一个纯 Python 包。它的名字就是当你引用它内部任何东西时需要用到的 Python 包名。 (比如 mysite.urls). mysite/init.py：一个空文件，告诉 Python 这个目录应该被认为是一个 Python 包。如果你是 Python 初学者，阅读官方文档中的 更多关于包的知识。 mysite/settings.py：Django 项目的配置文件。如果你想知道这个文件是如何工作的，请查看 Django settings 了解细节。 mysite/urls.py：Django 项目的 URL 声明，就像你网站的“目录”。阅读 URL调度器 文档来获取更多关于 URL 的内容。 mysite/wsgi.py：作为你的项目的运行在 WSGI 兼容的Web服务器上的入口。阅读 如何使用 WSGI 进行部署 了解更多细节。 用于开发的简易服务器 python manage.py runserver 8001(默认端口是8000) 输出如下： Performing system checks... System check identified no issues (0 silenced). You have unapplied migrations; your app may not work properly until they are applied. Run 'python manage.py migrate' to apply them. 二月 28, 2019 - 15:50:53 Django version 2.1, using settings 'mysite.settings' Starting development server at http://127.0.0.1:8000/ Quit the server with CONTROL-C. 创建应用 我们在的 manage.py 同级目录下创建投票应用。这样它就可以作为顶级模块导入，而不是 mysite 的子模块。 py manage.py startapp polls 创建了如下目录： polls/ __init__.py admin.py apps.py migrations/ __init__.py models.py tests.py views.py 项目 VS 应用 项目和应用有啥区别？应用是一个专门做某件事的网络应用程序——比如博客系统，或者公共记录的数据库，或者简单的投票程序。项目则是一个网站使用的配置和应用的集合。项目可以包含很多个应用。应用可以被很多个项目使用。 编写第一个视图 打开 polls/views.py from django.http import HttpResponse def index(request): return HttpResponse(\"Hello, world. You're at the polls index.\") 这是 Django 中最简单的视图。如果想看见效果，我们需要将一个 URL 映射到它——这就是我们需要 URLconf 的原因了。 为了创建 URLconf，请在 polls 目录里新建一个 urls.py 文件。 打开 polls/urls.py from django.urls import path from . import views urlpatterns = [ path('', views.index, name='index'), ] 下一步是要在根 URLconf 文件中指定我们创建的 polls.urls 模块。在 mysite/urls.py 文件的 urlpatterns 列表里插入一个 include() from django.contrib import admin from django.urls import include, path urlpatterns = [ path('polls/', include('polls.urls')), path('admin/', admin.site.urls), ] 函数 include() 允许引用其它 URLconfs。每当 Django 遇到 :func：~django.urls.include 时，它会截断与此项匹配的 URL 的部分，并将剩余的字符串发送到 URLconf 以供进一步处理。 我们设计 include() 的理念是使其可以即插即用。因为投票应用有它自己的 URLconf( polls/urls.py )，他们能够被放在 \"/polls/\" ， \"/fun_polls/\" ，\"/content/polls/\"，或者其他任何路径下，这个应用都能够正常工作。 当包括其它 URL 模式时你应该总是使用 include() ， admin.site.urls 是唯一例外。 现在把 index 视图添加进了 URLconf。可以验证是否正常工作，运行下面的命令: py manage.py runserver 访问 http://localhost:8000/polls/ 函数 path() 具有四个参数，两个必须参数：route 和 view，两个可选参数：kwargs 和 name。 route 是一个匹配 URL 的准则（类似正则表达式）。当 Django 响应一个请求时，它会从 urlpatterns 的第一项开始，按顺序依次匹配列表中的项，直到找到匹配的项。 这些准则不会匹配 GET 和 POST 参数或域名。例如，URLconf 在处理请求 https://www.example.com/myapp/ 时，它会尝试匹配 myapp/ 。处理请求 https://www.example.com/myapp/?page=3 时，也只会尝试匹配 myapp/。 view： 当 Django 找到了一个匹配的准则，就会调用这个特定的视图函数，并传入一个 HttpRequest 对象作为第一个参数，被“捕获”的参数以关键字参数的形式传入。稍后，我们会给出一个例子。 kwargs： 任意个关键字参数可以作为一个字典传递给目标视图函数 name： 为你的 URL 取名能使你在 Django 的任意地方唯一地引用它，尤其是在模板中。这个有用的特性允许你只改一个文件就能全局地修改某个 URL 模式。 使用mysql pip install pymysql 在Django的settings.py文件中设置如下： import pymysql # 一定要添加这两行！ pymysql.install_as_MySQLdb() DATABASES = { 'default': { 'ENGINE': 'django.db.backends.mysql', # 数据库引擎 'NAME': 'mysite', # 数据库名，先前创建的 'USER': 'root', # 用户名，可以自己创建用户 'PASSWORD': '****', # 密码 'HOST': '192.168.1.121', # mysql服务所在的主机ip 'PORT': '3306', # mysql服务端口 } } Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"python/Django去操作已经存在的数据库.html":{"url":"python/Django去操作已经存在的数据库.html","title":"Django去操作已经存在的数据库","keywords":"","body":"Django去操作已经存在的数据库 你有没有遇到过这种情况？ 数据库，各种表结构已经创建好了，甚至连数据都有了，此时，我要用Django管理这个数据库，ORM映射怎么办？？？ Django是最适合所谓的green-field开发，即从头开始一个新的项目 但是呢，Django也支持和以前遗留的数据库和应用相结合的。 Django的数据库层从Python代码生成SQL schemas。但是对于遗留的数据库，你已经用于SQL schemas，这种情况下你需要为你已经存在的数据库表写模型（为了使用数据库的API），幸运的是，Django自带有通过阅读你的数据库表规划来生成模型代码的辅助工具 manage.py inspectdb 1.Django默认使用的是sqllit数据库？如何使用MySQL数据库？ 修改setting.py文件 DATABASE = { 'default':{ 'ENGINE':'django.db.backends.mysql', 'NAME':'数据库名', 'HOST':'数据库地址', 'PORT':端口, 'USER':'用户名', 'PASSWORD':'密码', } } 由于Django内部链接MySQL数据库的时候默认的是使用MySQLdb的 但是Python3中没有这个模块 所以我们要去修改他的project同名文件夹下的init文件 import pymysql pymysql.install_as_MySQLdb() 然后呢，我们就需要根据数据库去自动生成新的models文件 python manage.py inspectdb #简单可以看一下自动映射成的models中的内容 导出并且去代替models.py python manage.py inspectdb > models.py 这样你就会发现在manage.py的同级目录下生成了一个models.py文件 使用这个models.py文件覆盖app中的models文件。 如果完成了以上的操作，生成的是一个不可修改/删除的models，修改meta class中的managed = True则可以去告诉django可以对数据库进行操作 此时，我们再去使models.py和数据库进行同步 python manage.py migrate 这个时候就已经大功告成了！ 然我们来验证一下： python manage.py shell 一些查询语句 嗯.....没毛病。 Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"辅助工具/在线工具.html":{"url":"辅助工具/在线工具.html","title":"在线工具","keywords":"","body":" 2md：将网页复制的内容转为md格式 本地部署方法：(本机已有apache环境) cd D:\\phpEnv\\www\\ git clone https://github.com/phodal/2md.git ./2md 浏览器打开http:localhost:81/2md Michaelromic            此页面修订于： 2019-05-15 09:02:48 "},"mysql/MySQL两种存储引擎_MyISAM和InnoDB简单总结.html":{"url":"mysql/MySQL两种存储引擎_MyISAM和InnoDB简单总结.html","title":"MySQL两种存储引擎:MyISAM和InnoDB简单总结","keywords":"","body":"MySQL两种存储引擎: MyISAM和InnoDB 简单总结 MyISAM是MySQL的默认数据库引擎（5.5版之前），由早期的ISAM（Indexed Sequential Access Method：有索引的顺序访问方法）所改良。虽然性能极佳，但却有一个缺点：不支持事务处理（transaction）。不过，在这几年的发展下，MySQL也导入了InnoDB（另一种数据库引擎），以强化参考完整性与并发违规处理机制，后来就逐渐取代MyISAM。 InnoDB，是MySQL的数据库引擎之一，为MySQL AB发布binary的标准之一。InnoDB由Innobase Oy公司所开发，2006年五月时由甲骨文公司并购。与传统的ISAM与MyISAM相比，InnoDB的最大特色就是支持了ACID兼容的事务（Transaction）功能，类似于PostgreSQL。目前InnoDB采用双轨制授权，一是GPL授权，另一是专有软件授权。 MyISAM和InnoDB两者之间有着明显区别，简单梳理如下: 1) 事务支持 MyISAM不支持事务，而InnoDB支持。InnoDB的AUTOCOMMIT默认是打开的，即每条SQL语句会默认被封装成一个事务，自动提交，这样会影响速度，所以最好是把多条SQL语句显示放在begin和commit之间，组成一个事务去提交。 MyISAM是非事务安全型的，而InnoDB是事务安全型的，默认开启自动提交，宜合并事务，一同提交，减小数据库多次提交导致的开销，大大提高性能。 2) 存储结构 MyISAM：每个MyISAM在磁盘上存储成三个文件。第一个文件的名字以表的名字开始，扩展名指出文件类型。.frm文件存储表定义。数据文件的扩展名为.MYD (MYData)。索引文件的扩展名是.MYI (MYIndex)。 InnoDB：所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件），InnoDB表的大小只受限于操作系统文件的大小，一般为2GB。 3) 存储空间 MyISAM：可被压缩，存储空间较小。支持三种不同的存储格式：静态表(默认，但是注意数据末尾不能有空格，会被去掉)、动态表、压缩表。 InnoDB：需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。 4) 可移植性、备份及恢复 MyISAM：数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。在备份和恢复时可单独针对某个表进行操作。 InnoDB：免费的方案可以是拷贝数据文件、备份 binlog，或者用 mysqldump，在数据量达到几十G的时候就相对痛苦了。 5) 事务支持 MyISAM：强调的是性能，每次查询具有原子性,其执行数度比InnoDB类型更快，但是不提供事务支持。 InnoDB：提供事务支持事务，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。 6) AUTO_INCREMENT MyISAM：可以和其他字段一起建立联合索引。引擎的自动增长列必须是索引，如果是组合索引，自动增长可以不是第一列，他可以根据前面几列进行排序后递增。 InnoDB：InnoDB中必须包含只有该字段的索引。引擎的自动增长列必须是索引，如果是组合索引也必须是组合索引的第一列。 7) 表锁差异 MyISAM：只支持表级锁，用户在操作myisam表时，select，update，delete，insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据。 InnoDB：支持事务和行级锁，是innodb的最大特色。行锁大幅度提高了多用户并发操作的新能。但是InnoDB的行锁，只是在WHERE的主键是有效的，非主键的WHERE都会锁全表的。 MyISAM锁的粒度是表级，而InnoDB支持行级锁定。简单来说就是, InnoDB支持数据行锁定，而MyISAM不支持行锁定，只支持锁定整个表。即MyISAM同一个表上的读锁和写锁是互斥的，MyISAM并发读写时如果等待队列中既有读请求又有写请求，默认写请求的优先级高，即使读请求先到，所以MyISAM不适合于有大量查询和修改并存的情况，那样查询进程会长时间阻塞。因为MyISAM是锁表，所以某项读操作比较耗时会使其他写进程饿死。 8) 全文索引 MyISAM：支持(FULLTEXT类型的)全文索引 InnoDB：不支持(FULLTEXT类型的)全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好。 全文索引是指对char、varchar和text中的每个词（停用词除外）建立倒排序索引。MyISAM的全文索引其实没啥用，因为它不支持中文分词，必须由使用者分词后加入空格再写到数据表里，而且少于4个汉字的词会和停用词一样被忽略掉。 另外，MyIsam索引和数据分离，InnoDB在一起，MyIsam天生非聚簇索引，最多有一个unique的性质，InnoDB的数据文件本身就是主键索引文件，这样的索引被称为“聚簇索引” 9) 表主键 MyISAM：允许没有任何索引和主键的表存在，索引都是保存行的地址。 InnoDB：如果没有设定主键或者非空唯一索引，就会自动生成一个6字节的主键(用户不可见)，数据是主索引的一部分，附加索引保存的是主索引的值。InnoDB的主键范围更大，最大是MyISAM的2倍。 10) 表的具体行数 MyISAM：保存有表的总行数，如果select count() from table;会直接取出出该值。 InnoDB：没有保存表的总行数(只能遍历)，如果使用select count() from table；就会遍历整个表，消耗相当大，但是在加了wehre条件后，myisam和innodb处理的方式都一样。 11) CURD操作 MyISAM：如果执行大量的SELECT，MyISAM是更好的选择。 InnoDB：如果你的数据执行大量的INSERT或UPDATE，出于性能方面的考虑，应该使用InnoDB表。DELETE 从性能上InnoDB更优，但DELETE FROM table时，InnoDB不会重新建立表，而是一行一行的删除，在innodb上如果要清空保存有大量数据的表，最好使用truncate table这个命令。 12) 外键 MyISAM：不支持 InnoDB：支持 13) 查询效率 没有where的count()使用MyISAM要比InnoDB快得多。因为MyISAM内置了一个计数器，count()时它直接从计数器中读，而InnoDB必须扫描全表。所以在InnoDB上执行count()时一般要伴随where，且where中要包含主键以外的索引列。为什么这里特别强调“主键以外”？因为InnoDB中primary index是和raw data存放在一起的，而secondary index则是单独存放，然后有个指针指向primary key。所以只是count()的话使用secondary index扫描更快，而primary key则主要在扫描索引同时要返回raw data时的作用较大。MyISAM相对简单，所以在效率上要优于InnoDB，小型应用可以考虑使用MyISAM。 通过上述的分析，基本上可以考虑使用InnoDB来替代MyISAM引擎了，原因是InnoDB自身很多良好的特点，比如事务支持、存储 过程、视图、行级锁定等等，在并发很多的情况下，相信InnoDB的表现肯定要比MyISAM强很多。另外，任何一种表都不是万能的，只用恰当的针对业务类型来选择合适的表类型，才能最大的发挥MySQL的性能优势。如果不是很复杂的Web应用，非关键应用，还是可以继续考虑MyISAM的，这个具体情况可以自己斟酌。 MyISAM和InnoDB两者的应用场景： 1) MyISAM管理非事务表。它提供高速存储和检索，以及全文搜索能力。如果应用中需要执行大量的SELECT查询，那么MyISAM是更好的选择。 2) InnoDB用于事务处理应用程序，具有众多特性，包括ACID事务支持。如果应用中需要执行大量的INSERT或UPDATE操作，则应该使用InnoDB，这样可以提高多用户并发操作的性能。 但是实际场景中，针对具体问题需要具体分析，一般而言可以遵循以下几个问题： 数据库是否有外键？ 是否需要事务支持？ 是否需要全文索引？ 数据库经常使用什么样的查询模式？在写多读少的应用中还是Innodb插入性能更稳定，在并发情况下也能基本，如果是对读取速度要求比较快的应用还是选MyISAM。 数据库的数据有多大？ 大尺寸倾向于innodb，因为事务日志，故障恢复。 Michaelromic            此页面修订于： 2019-05-15 09:02:48 "}}